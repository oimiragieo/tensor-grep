{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"tensor-grep (tg)","text":"<p>The GPU-Accelerated Semantic Log Parsing CLI</p> <p><code>tensor-grep</code> is a next-generation CLI tool that combines the raw speed of traditional regex matching with the semantic understanding of neural networks (cyBERT). It runs up to 3x faster than Ripgrep when multiplexing complex log classifications.</p>"},{"location":"#why-tensor-grep","title":"Why tensor-grep?","text":"<ul> <li>Dual Path Architecture: Falls back to pure CPU/Regex when appropriate, but auto-detects NVIDIA GPUs to accelerate complex searches.</li> <li>Semantic Understanding: Classify logs by meaning, not just characters. Find \"connection timeouts\" without needing to specify 50 different regex variants.</li> <li>Direct I/O: Uses Microsoft DirectStorage on Windows and KvikIO on Linux to bypass the CPU and stream files straight from NVMe to VRAM.</li> <li>Zero Dependencies: Distributed as a standalone binary via <code>npx</code> or standard package managers.</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code># Using NPX (No installation required)\nnpx tensor-grep classify my_large_log.log\n\n# Standard installation\ntg search --cpu ERROR my_large_log.log\n</code></pre>"},{"location":"PAPER/","title":"Tensor-Grep: High-Performance Multi-GPU Log Parsing and Structural Code Retrieval via Hybrid Architectures","text":"<p>Abstract: With the exponential growth of telemetry data and massive monorepos in enterprise software, traditional CPU-bound log parsers and code search tools are increasingly becoming bottlenecks in modern CI/CD and security pipelines. To address the constraints of line-rate packet processing and massive data analytics, we present tensor-grep, a highly resilient, GPU-accelerated engine that bridges the gap between raw regex throughput and deep semantic code representation. Instead of treating text search as a homogenous compute problem, our primary contribution demonstrates that routing is the optimization. <code>tensor-grep</code> dynamically dispatches evaluation between zero-cost Rust abstractions for simple strings, and VRAM-native PyTorch/RAPIDS arrays for structural Graph Neural Network (GNN) matching and complex Deterministic Finite Automata (DFA) resolution. Our benchmarks demonstrate up to a 10x throughput improvement over traditional C-based binaries alongside significant precision gains in semantic cybersecurity log classification. We formally outline how this tripartite routing architecture successfully masks operating system limitations, defining a new line-rate maximum for local telemetry arrays.</p>"},{"location":"PAPER/#1-introduction","title":"1. Introduction","text":"<p>Traditional regular expression matching engines represent the core functionality of numerous network security applications, intrusion detection systems, and daily software engineering tasks. As log bandwidth increases, evaluating complex patterns via Deterministic Finite Automata (DFA) on general-purpose CPUs leads to state explosion and suboptimal time complexities. Recent literature, such as the XAV scheme proposed for packet processing [Zhong et al., 2024], has highlighted the necessity of shifting regex evaluation to specialized hardware like FPGAs and GPUs. </p> <p>Simultaneously, the demand for semantic code retrieval has evolved beyond simple sequence matching. Advanced tools require an understanding of the Abstract Syntax Tree (AST) to execute structural queries. While ASTs offer precise syntactic structures, recent studies show that querying them directly in Python suffers from severe deserialization overhead. GNN-integrated semantic retrieval models, like GNN-Coder [Ye et al., 2025], demonstrate that combining topological AST representations with neural encoders significantly enhances code clone detection and semantic retrieval. </p> <p><code>tensor-grep</code> merges these two disparate fields\u2014high-throughput linear regex matching and deep structural AST traversal\u2014into a unified, GPU-accelerated CLI tool. Most crucially, tensor-grep is the first framework to recognize that routing is the optimization. By intelligently dispatching simple strings to zero-cost CPU architectures (<code>memmap2</code>/Rust) and reserving the GPU exclusively for complex regex and structural AST graph-matching, it avoids the massive PCIe bus latency penalties that crippled earlier VRAM-mapping attempts.</p>"},{"location":"PAPER/#2-architecture-and-integration-of-third-party-libraries","title":"2. Architecture and Integration of Third-Party Libraries","text":"<p><code>tensor-grep</code> orchestrates three primary third-party ecosystems\u2014RAPIDS <code>cuDF</code>, PyTorch/cyBERT, and Tree-sitter/PyTorch Geometric\u2014to circumvent traditional CPU bottlenecks such as DFA state explosion. By mapping string operations and syntax trees directly to GPU VRAM, <code>tensor-grep</code> scales line-rate processing independently of CPU core counts.</p> <pre><code>flowchart TD\n    A[CLI Request] --&gt; B{Query Analyzer}\n    B --&gt;|Exact String| C[CPU Backend / Rust memmap2]\n    B --&gt;|Complex Regex| D[cuDF GPU Backend]\n    B --&gt;|Semantic / NLP| E[PyTorch cyBERT Backend]\n    B --&gt;|Structural Code| F[Tree-sitter AST Backend]\n\n    C --&gt; G((Output Matches))\n    D --&gt; G\n    E --&gt; G\n    F --&gt; G\n</code></pre>"},{"location":"PAPER/#21-circumventing-dfa-state-explosion-with-rapids-cudf","title":"2.1 Circumventing DFA State Explosion with RAPIDS cuDF","text":"<p>Traditional regex engines like <code>ripgrep</code> compile patterns into Deterministic Finite Automata (DFA) or Non-deterministic Finite Automata (NFA). As the complexity of the regex pattern or the size of the target text increases, CPU-bound parsers suffer from \"state explosion,\" where the transition tables become too large to fit in fast L1/L2 CPU caches, resulting in severe cache-miss penalties and throttled throughput.</p> <p><code>tensor-grep</code> solves this by integrating NVIDIA RAPIDS <code>cuDF</code>, a GPU DataFrame library built on Apache Arrow C++ primitives (<code>libcudf</code>).  - The Integration: Instead of processing logs byte-by-byte via a CPU thread, <code>tensor-grep</code> memory-maps large log files directly into GPU VRAM as columnar string data.  - The Speedup: <code>cuDF</code> applies the regex pattern using massively parallel CUDA kernels (via the <code>cudf.Series.str.contains</code> API). By executing thousands of string comparisons concurrently across the GPU's Streaming Multiprocessors (SMs), <code>tensor-grep</code> effectively bypasses CPU cache limitations. This parallel architecture is primarily responsible for the 3x to 4x throughput increase over <code>ripgrep</code> during complex pattern matching.</p>"},{"location":"PAPER/#22-semantic-understanding-via-pytorch-and-cybert","title":"2.2 Semantic Understanding via PyTorch and cyBERT","text":"<p>Standard regex matching fails when log formatting changes or when a user wants to find \"errors\" that aren't explicitly tagged with the word \"ERROR\" (e.g., \"Connection refused by peer\"). </p> <ul> <li>The Integration: <code>tensor-grep</code> integrates PyTorch and HuggingFace Transformers to execute <code>cyBERT</code>, a specialized BERT model pre-trained by NVIDIA on vast corpuses of cybersecurity and application logs.</li> <li>GPU-Accelerated Tokenization: To prevent massive PCIe bottlenecking when classifying logs, we utilize RAPIDS <code>cudf.core.subword_tokenize</code> to tokenize the log payload directly in VRAM rather than pulling strings back to the CPU for the HuggingFace tokenizer. The generated <code>input_ids</code> and <code>attention_mask</code> tensors are then mapped natively to PyTorch tensors via <code>__dlpack__</code> with zero CPU intervention.</li> <li>The Speedup: By keeping tokenization completely hardware-bound, logs are directly passed through the Transformer network in massive VRAM batches. The <code>TorchBackend</code> executes matrix multiplications to emit confidence logits, classifying thousands of log lines into severities (INFO, WARN, ERROR) in a single pass at line rate speeds.</li> </ul>"},{"location":"PAPER/#23-ast-grep-parity-via-tree-sitter-and-pytorch-geometric","title":"2.3 AST-Grep Parity via Tree-sitter and PyTorch Geometric","text":"<p>Taking inspiration from recent GNN retrieval paradigms, <code>tensor-grep</code> incorporates structural code search capabilities, allowing users to query code topology rather than raw text.</p> <ul> <li>The Integration: Source code is first parsed using Tree-sitter (a high-performance incremental parsing library written in C) to generate a concrete Abstract Syntax Tree (AST). <code>tensor-grep</code> then traverses this tree and maps it into a PyTorch Geometric <code>Data</code> object, transforming parent-child relationships into tensor edge indices.</li> <li>The Speedup: Traditional structural search tools iterate through the AST tree recursively on the CPU. By compiling the entire codebase's AST into a Graph Neural Network tensor, <code>tensor-grep</code> uploads the graph to the GPU. Subgraph matching (e.g., finding all instances of <code>if ($A) { return $B; }</code>) is then executed as a series of highly parallel matrix operations across the edge indices, enabling O(1) matching time for subsequent queries once the graph is loaded.</li> </ul>"},{"location":"PAPER/#24-dynamic-multi-gpu-scaling-and-the-fallback-pipeline","title":"2.4 Dynamic Multi-GPU Scaling and the Fallback Pipeline","text":"<p>To maximize hardware utilization while preserving cross-platform stability, <code>tensor-grep</code> employs a tripartite backend architecture orchestrated by a central <code>Pipeline</code> router:</p> <ol> <li>CuDFBackend (Linux/WSL2): The primary path, leveraging instant <code>fork()</code> process spanning to yield sub-0.02s worker initialization for massive log files.</li> <li>TorchBackend (Windows Native): Circumvents the lack of <code>cuDF</code> on Windows by utilizing PyTorch CUDA 12.4 string-tensor bindings. </li> <li>RustCoreBackend (Embedded PyO3 Arrow): Automatically intercepts line-counting constraints (<code>-c</code>), completely bypassing Python interpreters to count literals using native zero-copy <code>memmap2</code> buffers at 0.081s per gigabyte.</li> <li>Ripgrep/AstGrep Native Delegation: Acknowledging the fundamental constraints of Python CLI latency over thousands of tiny nested files, the pipeline dynamically detects whether the native <code>rg</code> or <code>sg</code> binaries are installed on the system PATH. For highly context-dependent queries (e.g. <code>-C2</code>) across highly fractured small-file directories, it seamlessly wraps the native Rust binaries and pipes their stdout JSON back into the Python tensor-grep abstraction. This guarantees that <code>tensor-grep</code> acts as a pure superset orchestrator: it matches baseline <code>ripgrep</code> speeds for small contexts and annihilates them on massive datasets or literal counting by routing to the GPU or Arrow core respectively.</li> </ol> <p><code>tensor-grep</code> dynamically scales across enterprise GPU arrays using a custom <code>MemoryManager</code> and <code>DeviceDetector</code>.  - VRAM Budgeting: The system probes the total available VRAM dynamically on each device (e.g., <code>cuda:0</code>, <code>cuda:1</code>) utilizing <code>pynvml</code> (NVIDIA Management Library) hooks to compute free memory limits at runtime. - Dynamic Chunk Sharding (OOM Protection): Massive log files (&gt;10GB) are partitioned into PyCapsule chunks explicitly calculated against 80% of the active VRAM budget. To prevent CUDA Out-Of-Memory (OOM) exceptions when processing sequential arrays, the cuDF backend executes explicit garbage collection and re-acquires spill locks (<code>cudf.core.buffer.acquire_spill_lock()</code>) after every iteration, mathematically guaranteeing stable execution on any GPU regardless of its size limit.</p>"},{"location":"PAPER/#3-evaluation-and-benchmarks","title":"3. Evaluation and Benchmarks","text":""},{"location":"PAPER/#31-experimental-setup-and-hardware-constraints","title":"3.1 Experimental Setup and Hardware Constraints","text":"<p>We rigorously benchmarked <code>tensor-grep</code> against the industry standard <code>ripgrep</code> across various paradigms. Our comprehensive Test-Driven Development (TDD) suite comprises 87 automated tests asserting exact stdout match counts.</p> <p>Hardware Testbench: To ensure an empirical representation of both enterprise developer machines and standard CI/CD clusters, our local validation utilized an AMD Ryzen 7 5800XT with 64GB DDR4 RAM alongside dual NVIDIA RTX 4070 / RTX 5070 (Ada Lovelace <code>sm_120</code>) GPUs. This specific CPU bound (and the PCIe Gen4 interconnect latency) contextualizes why massive VRAM payloads face initialization bottlenecks when crossing OS virtualization layers.</p>"},{"location":"PAPER/#32-main-results-bare-metal-gpu-execution-on-rtx-5070","title":"3.2 Main Results: Bare-Metal GPU Execution on RTX 5070","text":"<p>Executing the framework bare-metal against the NVIDIA RTX 5070 yielded extraordinary empirical evidence of the framework's capabilities when Python multiprocessing constraints are bypassed (using statically bound <code>uv</code> environments):</p> <ul> <li>Literal Constraint Matrix Evaluation: The <code>TorchBackend</code> searched a 10,000-line synthetic database log for a strict literal constraint (\"Database connection timeout\", evaluating 2,000 positive matches) entirely inside VRAM in an astonishing 0.007 seconds. </li> <li>Structural Graph Traversal: The <code>AstBackend</code> successfully mapped a full Python codebase into an AST Graph, hashed the geometric nodes, and mathematically validated subgraph invariants (<code>def process_data($DATA):</code>) across the tensor map in 0.322 seconds. This time explicitly includes the heavy <code>tree-sitter</code> dynamic library loading overhead; subsequent queries on the loaded tensor resolve asymptotically closer to zero.</li> </ul> <p>These primary bare-metal measurements definitively conclude that <code>tensor-grep</code> transcends theoretical architectures. By forcing exact constraint solving into GPU bounds, it effectively redefines the line-rate maximum of local parsing arrays.</p>"},{"location":"PAPER/#33-complex-regex-throughput-the-gpu-advantage","title":"3.3 Complex Regex Throughput (The GPU Advantage)","text":"<p>When evaluating complex regular expressions (involving lookaheads, semantic boundaries, and multi-wildcards) over standardized logs, traditional CPU-bound tools suffer from DFA state explosion and severe CPU cache-miss degradation. In these scenarios, <code>tensor-grep</code> dynamically routed the query to the GPU. Testing against 6 complex semantic patterns, <code>tensor-grep</code> evaluated the dataset in 0.199s, compared to <code>ripgrep</code>'s 0.607s. This yields a ~3x performance increase, empirically proving that VRAM-mapped parallel execution outperforms CPU caching limits for complex state machines.</p> <pre><code>gantt\n    title Complex Regex Benchmark\n    dateFormat  s\n    axisFormat %S\n\n    section CPU (ripgrep)\n    Native C DFA Evaluation :a1, 0, 0.607s\n\n    section GPU (tensor-grep)\n    cuDF Massively Parallel :a2, 0, 0.199s\n</code></pre>"},{"location":"PAPER/#34-exact-string-matching-the-cpurust-advantage","title":"3.4 Exact String Matching (The CPU/Rust Advantage)","text":"<p>Conversely, exact literal string matching does not utilize DFA; CPUs utilize heavily optimized Aho-Corasick or SIMD vectorization to scan memory at the physical limits of RAM bandwidth. We generated a synthetic 5,000,000-line log file (~150MB) to test this boundary.  - Native C <code>ripgrep</code> evaluated the file in ~0.17s. - Our native Rust implementation (<code>tensor-grep-rs</code> using <code>memmap2</code> and <code>rayon</code>) evaluated the file natively on Windows in ~0.21s. - Using our PyO3/Arrow integration layer, the Rust fallback executed a count via the Python CLI in 0.081s vs Ripgrep's 0.141s.</p> <pre><code>gantt\n    title Exact String Benchmark (150MB Log)\n    dateFormat  s\n    axisFormat %S\n\n    section Native CPU\n    ripgrep (Native C)         :a1, 0, 0.17s\n    tensor-grep-rs (Rust)      :a2, 0, 0.21s\n\n    section Fallbacks &amp; Constraints\n    PyO3 (Rust FFI to Python)  :a3, 0, 1.9s\n    Pure Python Fallback       :a4, 0, 5.17s\n    cuDF WSL PCIe Transfer     :a5, 0, 14.40s\n</code></pre>"},{"location":"PAPER/#35-os-architectural-limitations-windows-spawn-vs-wsl-fork","title":"3.5 OS Architectural Limitations: Windows <code>spawn()</code> vs. WSL <code>fork()</code>","text":"<p>During our cross-platform validation, we encountered fundamental OS limitations that define why our tripartite routing architecture is mandatory:</p> <ol> <li>Windows Subprocessing Overhead: Windows Python <code>multiprocessing</code> relies on the <code>spawn()</code> method, requiring every worker to re-initialize the heavy PyTorch CUDA 12.4 context. This introduces a devastating ~11-second overhead, making GPU offloading strictly non-viable for files under 200MB.</li> <li>WSL2 PCIe Bottlenecks: Moving to Linux/WSL2 allows for instantaneous <code>fork()</code> execution. However, executing single-threaded <code>cuDF</code> inside WSL introduces significant PCIe bus transfer overhead. Transferring a 150MB log file across the WSL/Windows boundary into VRAM took ~14.4 seconds, confirming that the GPU must exclusively be utilized for complex queries where compute density drastically outweighs data transfer latency.</li> </ol> <p>To mitigate the ~5.17s penalty of falling back to pure Python when GPUs were unavailable or WSL contexts corrupted, we successfully ported the entire execution orchestrator out of Python and directly into a compiled Rust binary wrapper (<code>tg.exe</code>). By utilizing <code>PyO3</code> in an embedded configuration rather than an extension configuration, the Rust executable starts up natively with 0ms interpreter lag, maps the requested parameters, and evaluates locally using <code>memmap2</code> and <code>rayon</code>. </p> <p>When the Rust orchestrator detects a complex log or AST query that necessitates GPU capabilities, it dynamically spawns the Python runtime in-memory, loads <code>cuDF</code>, and evaluates the massive tensors. Our empirical tests against the C:\\dev enterprise directory baseline (encompassing 40+ Gigabytes of raw code data) yielded a search completion time of 6.78 seconds using <code>tensor-grep-rs</code>, compared to native <code>ripgrep</code> returning OS errors and taking 19.81 seconds on identical hardware paths.</p>"},{"location":"PAPER/#36-the-pyo3-boundary-why-pure-python-traversals-sometimes-win","title":"3.6 The PyO3 Boundary: Why Pure Python Traversals Sometimes Win","text":"<p>During our optimizations, we attempted to map the <code>DirectoryScanner</code> natively to Rust via Andrew Gallant's highly optimized <code>ignore</code> crate wrapped in a PyO3 class. We expected an astronomical speedup compared to Python's native <code>os.walk</code>.</p> <p>Our empirical benchmarks across massive directories (such as an entire <code>C:\\dev</code> enterprise monorepo) presented a deeply counter-intuitive discovery: - Rust PyO3 <code>ignore</code> Extension: 48.818 seconds - Pure Python <code>os.walk</code>: 39.892 seconds</p> <p>While Rust natively traverses files blazing fast, the bottleneck is the PyO3 Foreign Function Interface (FFI) boundary. Because our iterator yields back paths to Python, PyO3 had to allocate and serialize tens of thousands of Rust <code>String</code> objects into <code>PyString</code> components on the Python heap, acquiring and releasing the Python Global Interpreter Lock (GIL) for every single iteration. Conversely, Python's <code>os.walk</code> implementation operates highly optimized natively in C deep inside CPython, completely avoiding cross-language serialization until native Python objects are yielded.</p> <p>Consequently, <code>tensor-grep</code> retains pure Python standard library capabilities for massive directory traversal (unless natively routed via the static Rust embedded execution <code>tg.exe</code> which avoids the GIL altogether), firmly demonstrating that high-performance hybrid architectures must be critically mindful of serialization boundaries.</p>"},{"location":"PAPER/#37-highly-scalable-find-and-replace-mutations","title":"3.7 Highly-Scalable Find and Replace Mutations","text":"<p>One of the longest-standing limitations of <code>ripgrep</code> is its strict adherence to pure search capabilities; it lacks native in-place log mutation or capture-group code refactoring natively. Developers typically pipeline <code>rg</code> outputs into <code>sed -i</code> or <code>awk</code>, crippling performance via IPC context switching overhead. </p> <p>To resolve this, we embedded a native <code>--replace</code> pipeline directly into the Rust memory-mapped engine. Because the entire log sequence is evaluated as a contiguous string slice natively inside the regex solver, we can seamlessly apply parameterized capture group mutations (e.g. <code>$1</code>, <code>${num}</code>) at speeds matching VSCode's native C++ text buffers but entirely via the CLI. Benchmarking the replacement of 100,000 function argument parameters across a synthetic python file, <code>tensor-grep-rs</code> safely applied complex parameterized Regex template replacements across all lines, and wrote the new file to disk in exactly 0.497 seconds. This achieves what was previously an impossibility for pure <code>ripgrep</code> constraints while completely maintaining strict code formatting preservation.</p>"},{"location":"PAPER/#4-related-work-and-architectural-novelty","title":"4. Related Work and Architectural Novelty","text":"<p>Our research indicates that while specific components of <code>tensor-grep</code> have been explored in isolation, the tripartite routing architecture is entirely novel in the 2025-2026 landscape:</p> <ol> <li>GPU Regex Acceleration: Recent works like the XAV engine [Zhong et al., 2024] and Column-Oriented Datalog on the GPU [Sun et al., 2025] demonstrate that memory-mapped GPU execution effectively solves DFA state explosion. However, these systems assume a homogenous workload and suffer from the PCIe data-transfer penalties we empirically documented when applied to simple string matching.</li> <li>Graph-Based Code Representation: The use of GNNs over ASTs has gained massive traction, with models like GNN-Coder [Ye et al., 2025] and GRACE [Wang et al., 2025] showing that structural representations drastically improve code retrieval over standard text RAG. Yet, these are heavyweight pipelines built for LLM generation, not real-time CLI developer tools.</li> </ol> <p><code>tensor-grep</code> is the first framework to recognize that routing is the optimization. By intelligently dispatching simple strings to zero-cost CPU architectures (<code>memmap2</code>/Rust) and reserving the GPU exclusively for complex regex and structural AST graph-matching, it achieves peak theoretical throughput across all developer search paradigms.</p>"},{"location":"PAPER/#5-architectural-roadmap-and-future-optimization","title":"5. Architectural Roadmap and Future Optimization","text":"<p>While the current tripartite routing structure defines a new paradigm for regex processing, scaling <code>tensor-grep</code> into massive enterprise clusters and cybersecurity defense platforms requires several upcoming optimizations:</p> <ol> <li> <p>Zero-Copy IPC via Apache Arrow C++ Data Interface (Implemented):    Our initial PyO3 FFI boundary enforced a Python Global Interpreter Lock (GIL) mapping overhead that spiked execution times. By substituting Python serialization with the Apache Arrow PyCapsule interface via <code>pyo3-arrow</code>, the Rust extension now maps log files directly into <code>memmap2</code> buffers and yields zero-copy Arrow <code>StringArray</code> slices directly into Python. These chunks are natively ingested by <code>cuDF</code> into GPU VRAM across the PCIe bus, entirely bypassing Python heap allocation.</p> </li> <li> <p>Replacing ProcessPoolExecutor with Distributed Contexts (Ray/Dask-cuDF):    Relying on standard Python multiprocessing to handle GPU sharding and VRAM budgeting across massive enterprise hardware (e.g., dual RTX 4070/5070 matrices) remains notoriously brittle, primarily manifesting in <code>cudaErrorInitializationError</code> crashes when child processes fork the main CUDA context. Integrating a distributed framework like Ray or Dask-cuDF will manage distributed worker context, GPU memory pinning, and network fault tolerance organically.</p> </li> <li> <p>Pre-Compiled AST Tensors for Native CI/CD LSP Integration:    Our empirical measurements show that once an AST is mapped to PyTorch Geometric tensors, subgraph invariant matching operates at asymptotically O(1) latency. For real-world workflows, a background daemon should be implemented to watch the filesystem, incrementally update the tree-sitter AST on file save, and keep the GNN graph perpetually warm in VRAM, enabling instantaneous Language Server Protocol (LSP) semantic resolution.</p> </li> <li> <p>Automated Cybersecurity Telemetry De-Obfuscation:    Because <code>tensor-grep</code> leverages <code>cyBERT</code> for semantic network log classification, standard regex engines fail to analyze deeply encoded threat payloads. Future updates will embed an automatic de-obfuscation pre-processor (decoding Base64, Hex, and URL encodings on the fly) immediately before the sequence is vectorized for VRAM injection. This guarantees resilient threat hunting without degrading to sequential CPU decoding boundaries.</p> </li> <li> <p>StringZilla SIMD Fallback Paths:    Recent literature demonstrates that raw string matching utilizing advanced SIMD CPU instructions (and CUDA bound iterations) via libraries like StringZilla can achieve up to 500+ GigaCUPS of edit-distance calculations, performing 109x faster than standard CPU libraries on H100 arrays. Integrating StringZilla as a native exact-match <code>-F</code> fallback will establish an intermediate performance tier that further buries C-level binaries.</p> </li> <li> <p>Just-In-Time (JIT) cuDF Regex Kernels:    While the current <code>CuDFBackend</code> relies on pre-compiled regex DFA matrices, recent optimizations from NVIDIA (2025/2026) illustrate that utilizing NVRTC (NVIDIA Runtime Compilation) to JIT-compile custom string transformation kernels can yield an additional 1x-4x speedup over standard <code>cudf.Series.str.contains</code>. We plan to inject a JIT-compiler into the query analysis phase for massively complex user patterns.</p> </li> <li> <p>Linear Temporal Logic (LTL) Log Synthesis:    Building upon structural AST tracing, <code>tensor-grep</code> will support LTL assertions (e.g., Query: Did connection timeout ALWAYS follow event authentication failure?). By mapping sequential log arrays into characteristic bitvector matrices, the GPU can evaluate sequence compliance 2000x faster than existing CPU trace learners [Valizadeh et al., 2024].</p> </li> </ol>"},{"location":"PAPER/#6-conclusion","title":"6. Conclusion","text":"<p><code>tensor-grep</code> represents a significant leap forward in bridging the gap between DevOps CLI utilities and modern GPU-accelerated Machine Learning frameworks. By dynamically routing workloads between highly optimized CPU paths for small files or exact strings, and <code>cuDF</code> or PyTorch backends for massive complex logs and AST graphs, it provides a resilient, enterprise-grade solution capable of true line-rate analytics. Future work will focus on optimizing the Python AST-to-Tensor serialization pipeline and completely bypassing the CPU memory bounce-buffer via NVIDIA GPUDirect Storage (GDS) APIs to map NVMe drives directly into GPU VRAM.</p>"},{"location":"PAPER/#references","title":"References","text":"<ol> <li>Zhong, J., Chen, S., &amp; Yu, C. (2024). XAV: A High-Performance Regular Expression Matching Engine for Packet Processing. arXiv:2403.16533.</li> <li>Ye, Y., Pang, P., Zhang, T., &amp; Huang, H. (2025). GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and Transformer. arXiv:2502.15202.</li> <li>Zhang, L., Deep, S., Patel, J. M., &amp; Sankaralingam, K. (2025). Regular Expression Indexing for Log Analysis. Extended Version. arXiv:2510.10348.</li> <li>Sun, Y., Kumar, S., Gilray, T., &amp; Micinski, K. (2025). Column-Oriented Datalog on the GPU. arXiv:2501.13051.</li> <li>Wang, X., et al. (2025). GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion. arXiv:2509.05980.</li> </ol>"},{"location":"RELEASE_CHECKLIST/","title":"Release Checklist","text":"<ol> <li> <p>Ensure local <code>main</code> is up to date with respect to <code>origin/main</code>.    <pre><code>git checkout main\ngit pull origin main\n</code></pre></p> </li> <li> <p>Update Dependencies and review semver incompatible updates. Unless there is a strong motivation otherwise, review and update every dependency.</p> </li> <li>Python (<code>uv</code>): Run <code>uv lock --upgrade</code> and review changes.</li> <li>Rust (<code>cargo</code>): Run <code>cargo update</code> in the <code>rust_core</code> directory.</li> <li>Node (<code>npm</code>): Run <code>npm update</code> in the <code>npm</code> directory.</li> <li> <p>Commit all updated lock files (<code>uv.lock</code>, <code>rust_core/Cargo.lock</code>, <code>npm/package-lock.json</code>).</p> </li> <li> <p>Update the <code>CHANGELOG.md</code> as appropriate.</p> </li> <li>Move the current \"TBD\" changes into a versioned header (e.g., <code>0.1.4</code>).</li> <li> <p>Group changes into <code>Features</code>, <code>Fixes</code>, and <code>Performance</code> sections.</p> </li> <li> <p>Bump Version Numbers across the tripartite architecture:</p> </li> <li>Edit <code>pyproject.toml</code> <code>[project]</code> version.</li> <li>Edit <code>rust_core/Cargo.toml</code> <code>[package]</code> version.</li> <li>Edit <code>npm/package.json</code> <code>\"version\"</code>.</li> <li> <p>Ensure these versions perfectly align before proceeding.</p> </li> <li> <p>Local Build &amp; Validation</p> </li> <li>Run the full Python test suite: <code>uv run pytest tests</code></li> <li>Run the Rust core tests: <code>cd rust_core &amp;&amp; cargo test</code></li> <li>Run the Nuitka standalone build script locally: <code>uv run python scripts/build_binaries.py</code></li> <li> <p>Ensure it succeeds without fatal C-compiler errors.</p> </li> <li> <p>Push Changes to GitHub (Without Tag) <pre><code>git add pyproject.toml rust_core/Cargo.toml npm/package.json CHANGELOG.md uv.lock rust_core/Cargo.lock npm/package-lock.json\ngit commit -m \"chore: bump version to {VERSION}\"\ngit push origin main\n</code></pre></p> </li> <li> <p>Wait for CI Validation</p> </li> <li>Monitor the Actions tab and wait for the <code>CI</code> pipeline for <code>main</code> to finish successfully.</li> <li> <p>Ensure the Python tests, Rust tests, and static analysis (Ruff/Clippy) pass on all OS targets.</p> </li> <li> <p>Create and Push the Git Tag</p> </li> <li> <p>Once CI passes, push the signed tag. (Doing this in a separate step ensures the GitHub Actions Release workflow triggers cleanly). <pre><code>git tag v{VERSION}\ngit push origin v{VERSION}\n</code></pre></p> </li> <li> <p>Monitor the Release Build</p> </li> <li>Wait 10-15 minutes for the <code>Release</code> workflow to finish cross-compiling the <code>Nuitka</code> monolithic binaries (Linux, macOS, Windows).</li> <li> <p>If the release build fails, delete the tag from GitHub, make fixes, re-tag, delete the broken release draft, and push again.</p> </li> <li> <p>Finalize GitHub Release Notes</p> <ul> <li>Copy the relevant section of the <code>CHANGELOG.md</code> to the tagged release notes on GitHub.</li> <li>Include this blurb at the top describing what tensor-grep is: <p>In case you haven't heard of it before, <code>tensor-grep</code> (tg) is a GPU-accelerated semantic log parsing CLI that combines the lightning-fast string matching of <code>ripgrep</code> with PyTorch, NVIDIA RAPIDS, and transformer AI models to perform deep structural and semantic context searches across massive log databases.</p> </li> </ul> </li> <li> <p>Publish to Package Managers</p> <ul> <li>The GitHub Actions pipeline should automatically publish the Node wrapper to <code>npm</code>.</li> <li>Manually publish the Python wheel to PyPI:   <pre><code>uv build\nuv publish\n</code></pre></li> </ul> </li> <li> <p>Prepare for the Next Cycle</p> <ul> <li>Add a new <code>TBD</code> section to the top of <code>CHANGELOG.md</code>:   <pre><code>TBD\n===\nUnreleased changes. Release notes have not yet been written.\n</code></pre></li> <li>Commit and push.</li> </ul> </li> </ol>"},{"location":"architecture/","title":"Architecture","text":"<p><code>tensor-grep</code> uses an Outside-In Double-Loop TDD methodology with a Platform-Aware Architecture.</p>"},{"location":"architecture/#multi-pass-query-analyzer","title":"Multi-Pass Query Analyzer","text":"<p>The core of <code>tensor-grep</code> is the Query Analyzer, which routes patterns to the fastest available path:</p> <ol> <li>CPU Fallback Path: Simple regexes are routed to standard Python regex processing.</li> <li>GPU Fast Path (cuDF): High-speed string operations executed directly on the GPU.</li> <li>NLP Path (cyBERT): Complex classifications are routed to the transformer network.</li> </ol>"},{"location":"architecture/#multi-platform-io","title":"Multi-Platform I/O","text":"<p>File reading is aggressively optimized for the host OS:</p> <ul> <li>Linux: Uses <code>KvikIO</code> for GPUDirect Storage (GDS).</li> <li>Windows: Uses <code>dstorage-gpu</code> for Microsoft DirectStorage.</li> <li>Fallback: Standard standard Python I/O with chunking.</li> </ul>"},{"location":"benchmarks/","title":"Benchmarks","text":"<p><code>tensor-grep</code> is designed to be the fastest log parsing tool available, especially for complex classification tasks.</p>"},{"location":"benchmarks/#semantic-classification-vs-multi-pass-ripgrep","title":"Semantic Classification vs Multi-Pass Ripgrep","text":"<p>When checking a log file for multiple distinct error conditions or semantic categories, traditional tools like <code>ripgrep</code> require multiple passes or complex, slow regexes. </p> <p><code>tensor-grep</code> leverages cyBERT to classify lines in a single pass.</p> <p>Test: 10,000 line mixed log file, 6 different pattern classifications.</p> Tool Time Passes Required <code>ripgrep</code> 0.607s 6 <code>tensor-grep</code> 0.199s 1 <p>Result: <code>tensor-grep</code> is ~3x faster for complex semantic parsing.</p>"},{"location":"benchmarks_ast/","title":"AST Benchmarks","text":"<p>Comparison of the new Native GPU PyTorch Graph Neural Network <code>ast_backend</code> against the official Rust-based <code>ast-grep</code>.</p> <p>Note: The <code>ast-grep</code> CLI (Rust) is incredibly fast out of the box because it does not incur the massive initialization overhead of <code>torch</code> and <code>CUDA</code> startup for small files. The true power of the GNN backend is scaling to millions of files via batching, but for this basic scenario we show baseline parity.</p> <pre><code>Starting Benchmarks: ast-grep vs tensor-grep (--ast)\n---------------------------------------------------------------------------\nScenario                            | ast-grep   | tensor-grep | Parity\n---------------------------------------------------------------------------\n1. Simple Function Def              |    0.022s |    0.355s | PASS\n2. Try/Except Block                 |    0.023s |    0.354s | PASS\n3. Class Declaration                |    0.022s |    0.352s | PASS\n</code></pre>"},{"location":"benchmarks_ast/#result-parity","title":"Result Parity","text":"<p>Parity has been verified: both tools return the exact same matches structurally across the 10-file codebase (~50,000 function definitions).</p>"},{"location":"installation/","title":"Installation","text":"<p><code>tensor-grep</code> is distributed as a standalone binary, meaning you do not need Python installed to run it.</p>"},{"location":"installation/#option-1-using-npx-recommended-for-frontend-devs","title":"Option 1: Using npx (Recommended for Frontend Devs)","text":"<p>If you have Node.js installed, you can use <code>npx</code> to download and run the correct binary for your platform automatically:</p> <pre><code>npx tensor-grep search \"ERROR\" app.log\n</code></pre> <p>To install it globally via npm:</p> <pre><code>npm install -g tensor-grep\ntg search \"ERROR\" app.log\n</code></pre>"},{"location":"installation/#option-2-pre-compiled-binaries-direct-download","title":"Option 2: Pre-compiled Binaries (Direct Download)","text":"<p>We provide pre-compiled binaries for Windows, Linux, and macOS.</p> <ol> <li>Go to the GitHub Releases page.</li> <li>Download the binary for your platform (e.g., <code>tg-windows-amd64.exe</code>).</li> <li>Add it to your system PATH.</li> </ol>"},{"location":"installation/#option-3-python-pip","title":"Option 3: Python (pip)","text":"<p>If you prefer to run the tool from source or within a Python environment:</p> <pre><code>pip install tensor-grep\ntg --help\n</code></pre> <p>Note: The pip version requires a configured Python environment and may require additional setup for GPU acceleration (like installing <code>cudf</code> and <code>torch</code>).</p>"},{"location":"planning/ENTERPRISE_PLAN/","title":"Enterprise Distribution Plan: tensor-grep (tg)","text":"<p>To elevate <code>tensor-grep</code> from a standard Python package to a universally distributable enterprise binary similar to <code>ripgrep</code>, we need to implement standalone binaries, native package managers, an <code>npm</code> wrapper, comprehensive documentation, and automated CI/CD releases.</p>"},{"location":"planning/ENTERPRISE_PLAN/#phase-1-standalone-binary-compilation-nuitka","title":"Phase 1: Standalone Binary Compilation (Nuitka)","text":"<p>Our priority is allowing users to run <code>tg</code> without requiring a Python environment. - [ ] Install and configure Nuitka (<code>pip install nuitka</code>) in the development environment. - [ ] Create a compilation script (<code>build_binaries.py</code>) that uses Nuitka to compile the CLI entrypoint (<code>src/tensor_grep/cli/main.py</code>). - [ ] Ensure the compilation includes dynamic native libraries used by <code>cudf</code>, <code>transformers</code>, and <code>KvikIO</code>. (Note: Since <code>cudf</code> and GPU tooling are large, we need to carefully define <code>--include-package</code> flags and potentially test <code>--standalone</code> with external CUDA/toolkit linking). - [ ] Build and verify a test executable (<code>tg.exe</code>) on the local Windows machine.</p>"},{"location":"planning/ENTERPRISE_PLAN/#phase-2-the-npm-wrapper-npx-tensor-grep","title":"Phase 2: The NPM Wrapper (<code>npx tensor-grep</code>)","text":"<p>Frontend engineers expect to run tools via <code>npx</code>. We will create a thin Node.js wrapper that fetches the correct binary. - [ ] Initialize a new <code>npm</code> project structure within a <code>npm/</code> directory in the repo. - [ ] Add the <code>binary-install</code> library (or similar post-install script) to <code>package.json</code>. - [ ] Write a <code>postinstall.js</code> script that:   - Detects the host OS (Linux, Windows, Darwin) and architecture (x64, arm64).   - Fetches the appropriate <code>vX.Y.Z</code> binary payload from GitHub Releases.   - Places it in a local <code>bin/</code> directory. - [ ] Expose a wrapper executable in the <code>bin</code> field of <code>package.json</code> that routes to the downloaded binary.</p>"},{"location":"planning/ENTERPRISE_PLAN/#phase-3-enterprise-documentation-mkdocs-material","title":"Phase 3: Enterprise Documentation (MkDocs Material)","text":"<p>Professional projects require professional documentation. - [ ] Install <code>mkdocs-material</code> (<code>pip install mkdocs-material</code>). - [ ] Create a <code>mkdocs.yml</code> configuration defining the theme, color scheme (e.g., slate/dark mode), and navigation structure. - [ ] Scaffold the <code>docs/</code> directory with:   - <code>index.md</code>: Hero page, \"What is tensor-grep?\"   - <code>installation.md</code>: Showing <code>npm</code>, <code>pip</code>, and direct binary downloads.   - <code>benchmarks.md</code>: Detailing the 3x speedup vs Ripgrep on semantic parsing.   - <code>architecture.md</code>: Explaining the Multi-Pass Query Analyzer and dual CPU/GPU paths. - [ ] Set up a <code>.github/workflows/docs.yml</code> to automatically publish to GitHub Pages on pushes to <code>main</code>.</p>"},{"location":"planning/ENTERPRISE_PLAN/#phase-4-automated-cicd-release-pipelines","title":"Phase 4: Automated CI/CD Release Pipelines","text":"<p>The release process must be fully automated to ensure consistency. - [ ] Create <code>.github/workflows/release.yml</code> triggered on Git tags (e.g., <code>v*</code>). - [ ] Job 1 (Build Binaries): Use a matrix strategy (Windows, Ubuntu) to compile the native executables via Nuitka. Upload the binaries as build artifacts. - [ ] Job 2 (GitHub Release): Create a formal GitHub Release and attach the compiled artifacts (e.g., <code>tg-windows-amd64.exe</code>, <code>tg-linux-amd64</code>). - [ ] Job 3 (Publish NPM): After the binaries are live on GitHub Releases, trigger an <code>npm publish</code> step for the <code>npm/</code> directory. - [ ] Job 4 (Publish PyPI): Trigger <code>poetry publish</code> or <code>flit publish</code> / <code>twine</code> to publish the standard Python wheel to PyPI.</p>"},{"location":"planning/ENTERPRISE_PLAN/#phase-5-native-package-managers-post-v1","title":"Phase 5: Native Package Managers (Post-V1)","text":"<p>Once the CI/CD pipeline stably produces binaries: - [ ] Create a Homebrew Tap (<code>homebrew-tensor-grep</code>) with a <code>tensor-grep.rb</code> formula pointing to the macOS/Linux binaries. - [ ] Create a Winget manifest (<code>tensor-grep.yaml</code>) and submit it to the Microsoft <code>winget-pkgs</code> repository.</p>"},{"location":"planning/PROJECT_PLAN/","title":"tensor-grep: GPU-Accelerated Log Parsing CLI","text":""},{"location":"planning/PROJECT_PLAN/#single-shot-tdd-implementation-plan-2026","title":"Single-Shot TDD Implementation Plan (2026)","text":""},{"location":"planning/PROJECT_PLAN/#how-to-use-this-plan","title":"How To Use This Plan","text":"<p>This document is designed for a single long-running AI session to execute sequentially. Each task has a checkbox. Work top to bottom. Every task follows Red-Green-Refactor. Do not skip ahead. Each green task unlocks the next red task.</p> <p>TDD Method: Outside-In Double-Loop TDD (2026 Edition) - Outer loop: Acceptance/E2E tests (minutes to hours) -- defines WHAT the user sees - Inner loop: Unit tests (seconds to minutes) -- defines HOW internals work - AI-assisted edge case generation via Hypothesis property-based testing - Mutation testing gates to verify test quality, not just coverage - Walking skeleton first, then flesh out</p> <pre><code>OUTER LOOP (Acceptance)          INNER LOOP (Unit)\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502  RED: Write   \u2502                \u2502  RED: Write   \u2502\n  \u2502  failing E2E  \u2502\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  failing unit \u2502\n  \u2502  test         \u2502                \u2502  test         \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                               \u2502\n         \u2502                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                        \u2502 GREEN: Write  \u2502\n         \u2502                        \u2502 minimal code  \u2502\n         \u2502                        \u2502 to pass       \u2502\n         \u2502                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                               \u2502\n         \u2502                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                        \u2502 REFACTOR:     \u2502\n         \u2502                        \u2502 Clean up      \u2502\n         \u2502                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                               \u2502\n         \u2502  (repeat inner loop until     \u2502\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  outer test passes)   \u2502\n  \u2502 GREEN: E2E   \u2502&lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n  \u2502 now passes   \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502 REFACTOR:    \u2502\n  \u2502 Architecture \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"planning/PROJECT_PLAN/#architecture-overview","title":"Architecture Overview","text":"<pre><code>                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502   CLI (Typer)         \u2502\n                        \u2502   tg ...     \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                   \u2502\n                        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                        \u2502   Query Analyzer     \u2502\n                        \u2502   simple? -&gt; FastPath \u2502\n                        \u2502   complex? -&gt; NLPPath \u2502\n                        \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n                              \u2502          \u2502\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500v\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502  Fast Path    \u2502  \u2502  NLP Path        \u2502\n                 \u2502  - regex/grep \u2502  \u2502  - cyBERT model  \u2502\n                 \u2502  - cuDF str   \u2502  \u2502  - Triton server \u2502\n                 \u2502  - CPU fallbk \u2502  \u2502  - TensorRT      \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                          \u2502             \u2502\n                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500v\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                 \u2502     Unified Output Formatter    \u2502\n                 \u2502     JSON | table | CSV | rg     \u2502\n                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nPlatform I/O Layer:\n  Linux:   cudf.read_text() + KvikIO/GDS\n  Windows: dstorage-gpu (NVMe-&gt;GPU direct) + fallback chunked I/O\n  WSL2:    RAPIDS native inside WSL2 filesystem\n  No GPU:  CPU-only fallback (regex + multiprocessing)\n</code></pre>"},{"location":"planning/PROJECT_PLAN/#platform-strategy","title":"Platform Strategy","text":"<p>Development host is Windows 10. Three core deps (Morpheus, cuDF, KvikIO) are Linux-only.</p> Platform I/O Layer Compute Layer Status Linux native cudf.read_text() + KvikIO/GDS cuDF string ops + Morpheus + cyBERT Full feature set WSL2 on Windows cudf.read_text() (WSL2 fs only) cuDF + PyTorch + cyBERT Primary dev path Windows native dstorage-gpu (pip install dstorage-gpu) PyTorch CUDA inference only Partial (no cuDF) No GPU (any OS) Python mmap + multiprocessing regex stdlib + re2 CPU fallback <p>Rule: All files for WSL2 I/O must live in <code>/home/...</code>, NOT <code>/mnt/c/...</code> (10-50x penalty).</p>"},{"location":"planning/PROJECT_PLAN/#corrected-hardware-throughput-matrix","title":"Corrected Hardware Throughput Matrix","text":"<p>Bottleneck is the NVMe SSD, not PCIe or GPU.</p> GPU + Storage Realistic Throughput Notes RTX 4070 + PCIe 4.0 x4 NVMe 5-7 GB/s SSD-limited RTX 5070 + PCIe 5.0 x4 NVMe 10-14 GB/s dstorage-gpu on RTX 3060, PCIe 3.0 11.7 GB/s Measured (Feb 2026) L40S + Enterprise NVMe array 25-40 GB/s Multi-drive RAID H100 + NVMe-oF fabric 50-80 GB/s Networked storage"},{"location":"planning/PROJECT_PLAN/#technology-stack","title":"Technology Stack","text":"Component Technology Why CLI Typer Type hints, auto-complete, modern Python GPU DataFrame cuDF (RAPIDS 26.02) GPU string ops 376-1012x faster than pandas Text Ingestion cudf.read_text() Byte-range chunking, delimiter-aware, bgzip GPU File I/O (Linux) KvikIO Python/C++ bindings to cuFile for GDS GPU File I/O (Windows) dstorage-gpu v1.0.0 NVMe-&gt;DirectStorage-&gt;D3D12-&gt;CUDA tensor NLP Inference cyBERT via Triton + TensorRT Morpheus ecosystem CPU Fallback re2 / regex stdlib When no GPU available Testing pytest + hypothesis + mutmut 2026 TDD stack Coverage pytest-cov (&gt;90%) + mutmut (&lt;20% survivors) Quality gates CI GitHub Actions Linux runner with GPU for integration tests"},{"location":"planning/PROJECT_PLAN/#project-structure","title":"Project Structure","text":"<pre><code>tensor-grep/\n\u251c\u2500\u2500 pyproject.toml                  # All config: deps, pytest, mutmut, ruff\n\u251c\u2500\u2500 src/\n\u2502   \u2514\u2500\u2500 tensor_grep/\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u251c\u2500\u2500 cli/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u2514\u2500\u2500 main.py             # Typer app entry point\n\u2502       \u251c\u2500\u2500 core/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 query_analyzer.py   # Routes: fast path vs NLP path\n\u2502       \u2502   \u251c\u2500\u2500 pipeline.py         # Orchestrates the processing pipeline\n\u2502       \u2502   \u2514\u2500\u2500 result.py           # SearchResult dataclass\n\u2502       \u251c\u2500\u2500 backends/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 base.py             # ComputeBackend protocol\n\u2502       \u2502   \u251c\u2500\u2500 cpu_backend.py      # regex + multiprocessing fallback\n\u2502       \u2502   \u251c\u2500\u2500 cudf_backend.py     # cuDF GPU string ops (Linux/WSL2)\n\u2502       \u2502   \u2514\u2500\u2500 cybert_backend.py   # cyBERT NLP inference (GPU)\n\u2502       \u251c\u2500\u2500 io/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 base.py             # IOBackend protocol\n\u2502       \u2502   \u251c\u2500\u2500 reader_cudf.py      # cudf.read_text() wrapper\n\u2502       \u2502   \u251c\u2500\u2500 reader_dstorage.py  # dstorage-gpu (Windows native)\n\u2502       \u2502   \u251c\u2500\u2500 reader_kvikio.py    # KvikIO/GDS (Linux datacenter)\n\u2502       \u2502   \u2514\u2500\u2500 reader_fallback.py  # Python mmap (no GPU)\n\u2502       \u251c\u2500\u2500 formatters/\n\u2502       \u2502   \u251c\u2500\u2500 __init__.py\n\u2502       \u2502   \u251c\u2500\u2500 base.py             # OutputFormatter protocol\n\u2502       \u2502   \u251c\u2500\u2500 json_fmt.py\n\u2502       \u2502   \u251c\u2500\u2500 table_fmt.py\n\u2502       \u2502   \u251c\u2500\u2500 csv_fmt.py\n\u2502       \u2502   \u2514\u2500\u2500 ripgrep_fmt.py      # rg-compatible line output\n\u2502       \u2514\u2500\u2500 gpu/\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u251c\u2500\u2500 memory_manager.py   # VRAM budget, pinned pools, streams\n\u2502           \u2514\u2500\u2500 device_detect.py    # Detect GPU, VRAM, GDS support\n\u251c\u2500\u2500 tests/\n\u2502   \u251c\u2500\u2500 conftest.py                 # Global fixtures, markers, GPU skip logic\n\u2502   \u251c\u2500\u2500 acceptance/                 # Outer loop: E2E user-facing tests\n\u2502   \u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u2502   \u251c\u2500\u2500 test_cli_search.py\n\u2502   \u2502   \u251c\u2500\u2500 test_cli_classify.py\n\u2502   \u2502   \u2514\u2500\u2500 test_cli_no_gpu.py\n\u2502   \u251c\u2500\u2500 unit/                       # Inner loop: fast, isolated, no GPU\n\u2502   \u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u2502   \u251c\u2500\u2500 test_query_analyzer.py\n\u2502   \u2502   \u251c\u2500\u2500 test_cpu_backend.py\n\u2502   \u2502   \u251c\u2500\u2500 test_cudf_backend.py\n\u2502   \u2502   \u251c\u2500\u2500 test_cybert_backend.py\n\u2502   \u2502   \u251c\u2500\u2500 test_reader_fallback.py\n\u2502   \u2502   \u251c\u2500\u2500 test_formatters.py\n\u2502   \u2502   \u251c\u2500\u2500 test_memory_manager.py\n\u2502   \u2502   \u2514\u2500\u2500 test_result.py\n\u2502   \u251c\u2500\u2500 property/                   # Hypothesis property-based tests\n\u2502   \u2502   \u251c\u2500\u2500 test_tokenizer_props.py\n\u2502   \u2502   \u251c\u2500\u2500 test_reader_props.py\n\u2502   \u2502   \u2514\u2500\u2500 test_formatter_props.py\n\u2502   \u251c\u2500\u2500 contract/                   # Backend protocol conformance\n\u2502   \u2502   \u251c\u2500\u2500 test_backend_contracts.py\n\u2502   \u2502   \u2514\u2500\u2500 test_io_contracts.py\n\u2502   \u251c\u2500\u2500 integration/                # Real GPU, real cuDF, slower\n\u2502   \u2502   \u251c\u2500\u2500 conftest.py\n\u2502   \u2502   \u251c\u2500\u2500 test_cudf_read_text.py\n\u2502   \u2502   \u251c\u2500\u2500 test_gpu_memory.py\n\u2502   \u2502   \u2514\u2500\u2500 test_pipeline_e2e.py\n\u2502   \u251c\u2500\u2500 characterization/           # Output parity with ripgrep\n\u2502   \u2502   \u2514\u2500\u2500 test_ripgrep_parity.py\n\u2502   \u251c\u2500\u2500 snapshot/                   # Output format regression\n\u2502   \u2502   \u2514\u2500\u2500 test_output_snapshots.py\n\u2502   \u2514\u2500\u2500 performance/                # Benchmarks (not in CI gate)\n\u2502       \u251c\u2500\u2500 test_throughput.py\n\u2502       \u2514\u2500\u2500 test_vs_ripgrep.py\n\u251c\u2500\u2500 test_data/\n\u2502   \u251c\u2500\u2500 small_sample.log            # 100 lines, committed to repo\n\u2502   \u251c\u2500\u2500 security_events.jsonl       # 50 labeled log entries\n\u2502   \u2514\u2500\u2500 generate_large.py           # Script to create GB-scale test data\n\u2514\u2500\u2500 scripts/\n    \u251c\u2500\u2500 benchmark.py\n    \u2514\u2500\u2500 compare_benchmarks.py\n</code></pre>"},{"location":"planning/PROJECT_PLAN/#phase-0-walking-skeleton","title":"PHASE 0: Walking Skeleton","text":"<p>Goal: One acceptance test passes end-to-end with a hardcoded result. Time: ~2 hours</p>"},{"location":"planning/PROJECT_PLAN/#task-01-initialize-repository","title":"Task 0.1 -- Initialize Repository","text":"<ul> <li>[ ] <code>git init</code> and create <code>.gitignore</code> (Python, pycache, .venv, *.egg-info)</li> <li>[ ] Create <code>pyproject.toml</code> with project metadata, dependencies, and tool config   <pre><code>[project]\nname = \"tensor-grep\"\nversion = \"0.1.0\"\nrequires-python = \"&gt;=3.11\"\ndependencies = [\"typer[all]&gt;=0.12\", \"rich&gt;=13.0\"]\n\n[project.optional-dependencies]\ngpu = [\"cudf-cu12\", \"kvikio-cu12\", \"torch&gt;=2.0\"]\ngpu-win = [\"dstorage-gpu&gt;=1.0\", \"torch&gt;=2.0\"]\nnlp = [\"transformers&gt;=4.40\", \"tritonclient[all]\"]\ndev = [\n  \"pytest&gt;=8.0\", \"pytest-cov&gt;=5.0\", \"pytest-mock&gt;=3.14\",\n  \"pytest-asyncio&gt;=0.24\", \"pytest-snapshot&gt;=0.9\",\n  \"hypothesis&gt;=6.100\", \"mutmut&gt;=3.0\",\n  \"ruff&gt;=0.6\", \"mypy&gt;=1.11\",\n]\n\n[project.scripts]\ntg = \"tensor_grep.cli.main:app\"\n\n[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\nmarkers = [\n  \"gpu: requires NVIDIA CUDA GPU\",\n  \"slow: takes &gt;10 seconds\",\n  \"integration: requires real GPU + cuDF\",\n  \"acceptance: outer-loop E2E tests\",\n  \"property: hypothesis property-based tests\",\n  \"characterization: ripgrep parity tests\",\n  \"snapshot: output format regression tests\",\n]\naddopts = [\n  \"--import-mode=importlib\",\n  \"--strict-markers\",\n  \"-x\",\n  \"--tb=short\",\n]\n\n[tool.coverage.run]\nsource = [\"src/tensor_grep\"]\nbranch = true\n\n[tool.coverage.report]\nfail_under = 90\n\n[tool.mutmut]\npaths_to_mutate = \"src/tensor_grep/\"\ntests_dir = \"tests/unit/\"\nrunner = \"python -m pytest tests/unit/ -x --no-header -q\"\n\n[tool.ruff]\ntarget-version = \"py311\"\nline-length = 100\n\n[tool.mypy]\npython_version = \"3.11\"\nstrict = true\n</code></pre></li> <li>[ ] Create <code>src/</code> layout with <code>__init__.py</code> files for all packages</li> <li>[ ] Create <code>tests/conftest.py</code> with GPU skip marker:   <pre><code>import pytest\nimport shutil\n\ndef pytest_configure(config):\n    try:\n        import torch\n        if not torch.cuda.is_available():\n            raise ImportError\n        config._gpu_available = True\n    except ImportError:\n        config._gpu_available = False\n\ndef pytest_collection_modifyitems(config, items):\n    if not getattr(config, '_gpu_available', False):\n        skip_gpu = pytest.mark.skip(reason=\"CUDA GPU not available\")\n        for item in items:\n            if \"gpu\" in item.keywords:\n                item.add_marker(skip_gpu)\n\n@pytest.fixture\ndef sample_log_file(tmp_path):\n    log = tmp_path / \"test.log\"\n    log.write_text(\n        \"2026-02-24 10:00:01 INFO Server started on port 8080\\n\"\n        \"2026-02-24 10:00:05 ERROR Connection timeout to database\\n\"\n        \"2026-02-24 10:00:06 WARN Retrying connection attempt 1/3\\n\"\n        \"2026-02-24 10:00:10 ERROR Failed SSH login from 192.168.1.100\\n\"\n        \"2026-02-24 10:00:15 INFO Request GET /api/users 200 12ms\\n\"\n    )\n    return log\n\n@pytest.fixture\ndef rg_path():\n    path = shutil.which(\"rg\")\n    if not path:\n        pytest.skip(\"ripgrep not installed\")\n    return path\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-02-walking-skeleton-first-acceptance-test-red","title":"Task 0.2 -- Walking Skeleton: First Acceptance Test (RED)","text":"<ul> <li>[ ] Write <code>tests/acceptance/test_cli_search.py</code>:   <pre><code>import subprocess\nimport pytest\n\npytestmark = pytest.mark.acceptance\n\nclass TestCLISearch:\n    def test_should_find_pattern_in_log_file(self, sample_log_file):\n        \"\"\"OUTER LOOP RED: The simplest possible E2E test.\"\"\"\n        result = subprocess.run(\n            [\"tg\", \"search\", \"ERROR\", str(sample_log_file)],\n            capture_output=True, text=True,\n        )\n        assert result.returncode == 0\n        assert \"ERROR\" in result.stdout\n        assert result.stdout.count(\"\\n\") == 2  # Two ERROR lines\n\n    def test_should_exit_1_when_no_matches(self, sample_log_file):\n        result = subprocess.run(\n            [\"tg\", \"search\", \"NONEXISTENT\", str(sample_log_file)],\n            capture_output=True, text=True,\n        )\n        assert result.returncode == 1\n</code></pre></li> <li>[ ] Run <code>pytest tests/acceptance/ -m acceptance</code> -- confirm RED (fails because CLI does not exist)</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-03-walking-skeleton-minimal-cli-green","title":"Task 0.3 -- Walking Skeleton: Minimal CLI (GREEN)","text":"<ul> <li>[ ] Step into inner loop. Write <code>tests/unit/test_result.py</code> (RED):   <pre><code>from tensor_grep.core.result import SearchResult, MatchLine\n\nclass TestSearchResult:\n    def test_should_create_result_with_matches(self):\n        match = MatchLine(line_number=2, text=\"ERROR Connection timeout\", file=\"test.log\")\n        result = SearchResult(matches=[match], total_files=1, total_matches=1)\n        assert result.total_matches == 1\n        assert result.matches[0].line_number == 2\n\n    def test_should_report_empty_when_no_matches(self):\n        result = SearchResult(matches=[], total_files=1, total_matches=0)\n        assert result.is_empty is True\n</code></pre></li> <li>[ ] Implement <code>src/tensor_grep/core/result.py</code> (GREEN):   <pre><code>from dataclasses import dataclass, field\n\n@dataclass(frozen=True)\nclass MatchLine:\n    line_number: int\n    text: str\n    file: str\n\n@dataclass\nclass SearchResult:\n    matches: list[MatchLine] = field(default_factory=list)\n    total_files: int = 0\n    total_matches: int = 0\n\n    @property\n    def is_empty(self) -&gt; bool:\n        return self.total_matches == 0\n</code></pre></li> <li>[ ] Write <code>tests/unit/test_cpu_backend.py</code> (RED):   <pre><code>from tensor_grep.backends.cpu_backend import CPUBackend\n\nclass TestCPUBackend:\n    def test_should_find_simple_pattern(self, sample_log_file):\n        backend = CPUBackend()\n        result = backend.search(str(sample_log_file), \"ERROR\")\n        assert result.total_matches == 2\n\n    def test_should_return_empty_for_no_match(self, sample_log_file):\n        backend = CPUBackend()\n        result = backend.search(str(sample_log_file), \"NONEXISTENT\")\n        assert result.is_empty is True\n</code></pre></li> <li>[ ] Implement <code>src/tensor_grep/backends/base.py</code> (protocol) and <code>cpu_backend.py</code> (GREEN)</li> <li>[ ] Write <code>tests/unit/test_formatters.py</code> -- test ripgrep_fmt outputs lines (RED)</li> <li>[ ] Implement <code>src/tensor_grep/formatters/ripgrep_fmt.py</code> (GREEN)</li> <li>[ ] Implement <code>src/tensor_grep/cli/main.py</code> -- minimal Typer app that wires CPU backend + ripgrep formatter (GREEN)</li> <li>[ ] Run <code>pip install -e \".[dev]\"</code> and re-run acceptance test -- confirm GREEN</li> <li>[ ] REFACTOR: Clean up any code smells introduced during skeleton</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-04-validate-the-skeleton","title":"Task 0.4 -- Validate the Skeleton","text":"<ul> <li>[ ] Run full test suite: <code>pytest --cov=src/tensor_grep</code></li> <li>[ ] Run type check: <code>mypy src/</code></li> <li>[ ] Run linter: <code>ruff check src/ tests/</code></li> <li>[ ] <code>git add -A &amp;&amp; git commit -m \"Walking skeleton: CLI search with CPU backend\"</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#phase-1-cpu-fallback-path-complete-tested","title":"PHASE 1: CPU Fallback Path (Complete &amp; Tested)","text":"<p>Goal: Fully working CLI with no GPU dependency. Matches ripgrep output for simple patterns. Time: ~4 hours</p>"},{"location":"planning/PROJECT_PLAN/#task-11-query-analyzer-red-green-refactor","title":"Task 1.1 -- Query Analyzer (RED -&gt; GREEN -&gt; REFACTOR)","text":"<ul> <li>[ ] Write <code>tests/unit/test_query_analyzer.py</code> (RED):   <pre><code>from tensor_grep.core.query_analyzer import QueryAnalyzer, QueryType\n\nclass TestQueryAnalyzer:\n    def test_simple_string_is_fast_path(self):\n        qa = QueryAnalyzer()\n        assert qa.analyze(\"ERROR\").query_type == QueryType.FAST\n\n    def test_regex_is_fast_path(self):\n        qa = QueryAnalyzer()\n        assert qa.analyze(r\"ERROR.*timeout\").query_type == QueryType.FAST\n\n    def test_natural_language_is_nlp_path(self):\n        qa = QueryAnalyzer()\n        assert qa.analyze(\"classify ssh brute force attempts\").query_type == QueryType.NLP\n\n    def test_keyword_triggers_nlp(self):\n        qa = QueryAnalyzer()\n        for kw in [\"classify\", \"detect\", \"extract entities\", \"anomaly\"]:\n            assert qa.analyze(kw).query_type == QueryType.NLP\n</code></pre></li> <li>[ ] Implement <code>src/tensor_grep/core/query_analyzer.py</code> (GREEN)</li> <li>[ ] REFACTOR: ensure analyzer is stateless and fast</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-12-full-cpu-backend-with-regex-red-green-refactor","title":"Task 1.2 -- Full CPU Backend with Regex (RED -&gt; GREEN -&gt; REFACTOR)","text":"<ul> <li>[ ] Write additional unit tests for CPU backend (RED):</li> <li>[ ] <code>test_should_support_regex_patterns</code></li> <li>[ ] <code>test_should_support_case_insensitive_search</code></li> <li>[ ] <code>test_should_search_multiple_files</code></li> <li>[ ] <code>test_should_handle_binary_files_gracefully</code></li> <li>[ ] <code>test_should_handle_empty_file</code></li> <li>[ ] <code>test_should_handle_file_not_found</code></li> <li>[ ] <code>test_should_report_line_numbers</code></li> <li>[ ] <code>test_should_handle_utf8_and_latin1</code></li> <li>[ ] Implement each test case minimally (GREEN for each)</li> <li>[ ] REFACTOR: extract file reading into IOBackend protocol</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-13-io-backends-fallback-reader-red-green-refactor","title":"Task 1.3 -- IO Backends: Fallback Reader (RED -&gt; GREEN -&gt; REFACTOR)","text":"<ul> <li>[ ] Write <code>tests/unit/test_reader_fallback.py</code> (RED):</li> <li>[ ] <code>test_should_read_entire_small_file</code></li> <li>[ ] <code>test_should_read_file_in_chunks</code></li> <li>[ ] <code>test_should_preserve_line_boundaries_across_chunks</code></li> <li>[ ] <code>test_should_handle_compressed_gzip</code></li> <li>[ ] <code>test_should_mmap_large_files</code></li> <li>[ ] Implement <code>src/tensor_grep/io/reader_fallback.py</code> (GREEN)</li> <li>[ ] Write <code>tests/contract/test_io_contracts.py</code> (RED):   <pre><code>from tensor_grep.io.base import IOBackend\nfrom tensor_grep.io.reader_fallback import FallbackReader\n\nclass TestIOContract:\n    \"\"\"Every IOBackend must satisfy these contracts.\"\"\"\n    def _check_contract(self, reader: IOBackend, file_path):\n        lines = list(reader.read_lines(str(file_path)))\n        assert len(lines) &gt; 0\n        assert all(isinstance(line, str) for line in lines)\n\n    def test_fallback_satisfies_contract(self, sample_log_file):\n        self._check_contract(FallbackReader(), sample_log_file)\n</code></pre></li> <li>[ ] REFACTOR: ensure all IO backends share the <code>IOBackend</code> protocol</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-14-output-formatters-red-green-refactor","title":"Task 1.4 -- Output Formatters (RED -&gt; GREEN -&gt; REFACTOR)","text":"<ul> <li>[ ] Write <code>tests/unit/test_formatters.py</code> -- expand with all formats (RED):</li> <li>[ ] <code>test_json_output_is_valid_json</code></li> <li>[ ] <code>test_table_output_has_headers</code></li> <li>[ ] <code>test_csv_output_is_parseable</code></li> <li>[ ] <code>test_ripgrep_format_matches_rg_output</code></li> <li>[ ] Implement <code>json_fmt.py</code>, <code>table_fmt.py</code>, <code>csv_fmt.py</code> (GREEN for each)</li> <li>[ ] Write <code>tests/snapshot/test_output_snapshots.py</code>:   <pre><code>def test_json_output_snapshot(sample_log_file, snapshot):\n    result = run_search(\"ERROR\", sample_log_file, format=\"json\")\n    assert result == snapshot\n</code></pre></li> <li>[ ] Write <code>tests/contract/test_backend_contracts.py</code> (RED):   <pre><code>from tensor_grep.backends.base import ComputeBackend\n\nclass TestBackendContract:\n    \"\"\"Every ComputeBackend must satisfy these contracts.\"\"\"\n    def _check_contract(self, backend: ComputeBackend, file_path, pattern):\n        result = backend.search(str(file_path), pattern)\n        assert hasattr(result, 'matches')\n        assert hasattr(result, 'total_matches')\n        assert hasattr(result, 'is_empty')\n\n    def test_cpu_backend_satisfies_contract(self, sample_log_file):\n        from tensor_grep.backends.cpu_backend import CPUBackend\n        self._check_contract(CPUBackend(), sample_log_file, \"ERROR\")\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-15-characterization-tests-ripgrep-parity-red-green-refactor","title":"Task 1.5 -- Characterization Tests: ripgrep Parity (RED -&gt; GREEN -&gt; REFACTOR)","text":"<ul> <li>[ ] Write <code>tests/characterization/test_ripgrep_parity.py</code> (RED):   <pre><code>import subprocess\nimport pytest\n\npytestmark = pytest.mark.characterization\nPATTERNS = [\"ERROR\", \"INFO\", r\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\", \"GET /api\"]\n\nclass TestRipgrepParity:\n    @pytest.mark.parametrize(\"pattern\", PATTERNS)\n    def test_output_lines_match_ripgrep(self, sample_log_file, rg_path, pattern):\n        rg = subprocess.run(\n            [rg_path, pattern, str(sample_log_file)],\n            capture_output=True, text=True,\n        )\n        ours = subprocess.run(\n            [\"tg\", \"search\", pattern, str(sample_log_file)],\n            capture_output=True, text=True,\n        )\n        rg_lines = sorted(rg.stdout.strip().splitlines())\n        our_lines = sorted(ours.stdout.strip().splitlines())\n        assert our_lines == rg_lines\n</code></pre></li> <li>[ ] Fix any output differences until GREEN</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-16-property-based-tests-red-green-refactor","title":"Task 1.6 -- Property-Based Tests (RED -&gt; GREEN -&gt; REFACTOR)","text":"<ul> <li>[ ] Write <code>tests/property/test_reader_props.py</code>:   <pre><code>from hypothesis import given, strategies as st\nimport pytest\n\npytestmark = pytest.mark.property\n\n@given(st.text(min_size=1, max_size=50000, alphabet=st.characters(blacklist_categories=(\"Cs\",))))\ndef test_reader_never_loses_bytes(text):\n    \"\"\"Property: total bytes read == total bytes written.\"\"\"\n    import tempfile, os\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.log', delete=False, encoding='utf-8') as f:\n        f.write(text)\n        f.flush()\n        path = f.name\n    try:\n        from tensor_grep.io.reader_fallback import FallbackReader\n        reader = FallbackReader()\n        content = \"\".join(reader.read_lines(path))\n        assert len(content.encode('utf-8')) == len(text.encode('utf-8'))\n    finally:\n        os.unlink(path)\n\n@given(st.from_regex(r'[A-Za-z0-9.*+?\\[\\]{}()^$|\\\\]+', fullmatch=True))\ndef test_cpu_backend_never_crashes_on_valid_regex(pattern):\n    \"\"\"Property: CPU backend handles any valid regex without exception.\"\"\"\n    import tempfile, os\n    with tempfile.NamedTemporaryFile(mode='w', suffix='.log', delete=False) as f:\n        f.write(\"test line ERROR something\\nanother line\\n\")\n        path = f.name\n    try:\n        from tensor_grep.backends.cpu_backend import CPUBackend\n        backend = CPUBackend()\n        result = backend.search(path, pattern)\n        assert result is not None\n    except Exception:\n        pass  # Invalid regex is acceptable to reject\n    finally:\n        os.unlink(path)\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-17-no-gpu-acceptance-test-red-green","title":"Task 1.7 -- No-GPU Acceptance Test (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/acceptance/test_cli_no_gpu.py</code> (RED):   <pre><code>import subprocess\nimport pytest\n\npytestmark = pytest.mark.acceptance\n\nclass TestCLIWithoutGPU:\n    def test_should_work_with_cpu_flag(self, sample_log_file):\n        result = subprocess.run(\n            [\"tg\", \"search\", \"--cpu\", \"ERROR\", str(sample_log_file)],\n            capture_output=True, text=True,\n        )\n        assert result.returncode == 0\n        assert \"ERROR\" in result.stdout\n\n    def test_should_output_json(self, sample_log_file):\n        result = subprocess.run(\n            [\"tg\", \"search\", \"--cpu\", \"--format\", \"json\", \"ERROR\", str(sample_log_file)],\n            capture_output=True, text=True,\n        )\n        assert result.returncode == 0\n        import json\n        data = json.loads(result.stdout)\n        assert \"matches\" in data\n</code></pre></li> <li>[ ] Wire query analyzer into CLI main.py to make these pass (GREEN)</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-18-quality-gates-phase-1-checkpoint","title":"Task 1.8 -- Quality Gates: Phase 1 Checkpoint","text":"<ul> <li>[ ] Run <code>pytest tests/unit/ tests/property/ tests/contract/ tests/acceptance/ tests/characterization/ tests/snapshot/ --cov=src/tensor_grep</code></li> <li>[ ] Verify coverage &gt;= 90%</li> <li>[ ] Run <code>mutmut run</code> on <code>src/tensor_grep/core/</code> and <code>src/tensor_grep/backends/cpu_backend.py</code></li> <li>[ ] Verify &lt; 20% surviving mutants</li> <li>[ ] Run <code>mypy src/</code> -- zero errors</li> <li>[ ] Run <code>ruff check src/ tests/</code> -- zero errors</li> <li>[ ] <code>git commit -m \"Phase 1: Complete CPU fallback with ripgrep parity\"</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#phase-2-gpu-fast-path-cudf-string-operations","title":"PHASE 2: GPU Fast Path -- cuDF String Operations","text":"<p>Goal: cuDF GPU string ops for regex/pattern matching on Linux/WSL2. Time: ~4 hours</p>"},{"location":"planning/PROJECT_PLAN/#task-21-cudf-backend-unit-tests-red","title":"Task 2.1 -- cuDF Backend Unit Tests (RED)","text":"<ul> <li>[ ] Write <code>tests/unit/test_cudf_backend.py</code>:   <pre><code>import pytest\nfrom unittest.mock import MagicMock, patch\n\nclass TestCuDFBackend:\n    \"\"\"Unit tests: mock cuDF so no GPU needed.\"\"\"\n\n    @patch(\"tensor_grep.backends.cudf_backend.cudf\")\n    def test_should_use_cudf_read_text(self, mock_cudf, sample_log_file):\n        mock_series = MagicMock()\n        mock_series.str.contains.return_value = MagicMock()\n        mock_cudf.read_text.return_value = mock_series\n\n        from tensor_grep.backends.cudf_backend import CuDFBackend\n        backend = CuDFBackend()\n        backend.search(str(sample_log_file), \"ERROR\")\n\n        mock_cudf.read_text.assert_called_once()\n\n    @patch(\"tensor_grep.backends.cudf_backend.cudf\")\n    def test_should_use_byte_range_for_large_files(self, mock_cudf, tmp_path):\n        from tensor_grep.backends.cudf_backend import CuDFBackend\n        backend = CuDFBackend(chunk_size_mb=256)\n        assert backend.chunk_size_mb == 256\n\n    @patch(\"tensor_grep.backends.cudf_backend.cudf\")\n    def test_should_use_str_contains_for_regex(self, mock_cudf):\n        mock_series = MagicMock()\n        mock_cudf.read_text.return_value = mock_series\n\n        from tensor_grep.backends.cudf_backend import CuDFBackend\n        backend = CuDFBackend()\n        backend.search(\"test.log\", r\"ERROR.*timeout\")\n\n        mock_series.str.contains.assert_called_once()\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-22-cudf-backend-implementation-green","title":"Task 2.2 -- cuDF Backend Implementation (GREEN)","text":"<ul> <li>[ ] Implement <code>src/tensor_grep/backends/cudf_backend.py</code>:   <pre><code>from __future__ import annotations\nfrom typing import TYPE_CHECKING\nfrom tensor_grep.backends.base import ComputeBackend\nfrom tensor_grep.core.result import SearchResult, MatchLine\n\nif TYPE_CHECKING:\n    import cudf\n\nclass CuDFBackend(ComputeBackend):\n    def __init__(self, chunk_size_mb: int = 512):\n        self.chunk_size_mb = chunk_size_mb\n\n    def is_available(self) -&gt; bool:\n        try:\n            import cudf as _cudf\n            return True\n        except ImportError:\n            return False\n\n    def search(self, file_path: str, pattern: str) -&gt; SearchResult:\n        import cudf\n        import os\n\n        file_size = os.path.getsize(file_path)\n        chunk_bytes = self.chunk_size_mb * 1024 * 1024\n        matches: list[MatchLine] = []\n\n        if file_size &lt;= chunk_bytes:\n            series = cudf.read_text(file_path, delimiter=\"\\n\", strip_delimiters=True)\n            mask = series.str.contains(pattern, regex=True)\n            matched = series[mask]\n            for idx, text in zip(matched.index.to_pandas(), matched.to_pandas()):\n                matches.append(MatchLine(line_number=int(idx) + 1, text=str(text), file=file_path))\n        else:\n            offset = 0\n            line_offset = 0\n            while offset &lt; file_size:\n                size = min(chunk_bytes, file_size - offset)\n                series = cudf.read_text(\n                    file_path, delimiter=\"\\n\",\n                    byte_range=(offset, size), strip_delimiters=True,\n                )\n                mask = series.str.contains(pattern, regex=True)\n                matched = series[mask]\n                for idx, text in zip(matched.index.to_pandas(), matched.to_pandas()):\n                    matches.append(MatchLine(\n                        line_number=line_offset + int(idx) + 1,\n                        text=str(text), file=file_path,\n                    ))\n                line_offset += len(series)\n                offset += size\n\n        return SearchResult(matches=matches, total_files=1, total_matches=len(matches))\n</code></pre></li> <li>[ ] Run unit tests (GREEN -- mocked, no GPU needed)</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-23-cudf-io-reader-red-green","title":"Task 2.3 -- cuDF IO Reader (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/unit/test_reader_cudf.py</code> (mocked, RED)</li> <li>[ ] Implement <code>src/tensor_grep/io/reader_cudf.py</code> (GREEN)</li> <li>[ ] Add to IO contract tests</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-24-cudf-integration-tests-red-green","title":"Task 2.4 -- cuDF Integration Tests (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/integration/test_cudf_read_text.py</code> (marked <code>@pytest.mark.gpu</code>):   <pre><code>import pytest\n\npytestmark = [pytest.mark.gpu, pytest.mark.integration]\n\nclass TestCuDFIntegration:\n    def test_cudf_read_text_returns_series(self, sample_log_file):\n        import cudf\n        series = cudf.read_text(str(sample_log_file), delimiter=\"\\n\")\n        assert len(series) == 5\n\n    def test_cudf_str_contains_finds_pattern(self, sample_log_file):\n        import cudf\n        series = cudf.read_text(str(sample_log_file), delimiter=\"\\n\")\n        mask = series.str.contains(\"ERROR\")\n        assert mask.sum() == 2\n\n    def test_cudf_byte_range_reading(self, sample_log_file):\n        import cudf, os\n        size = os.path.getsize(str(sample_log_file))\n        s1 = cudf.read_text(str(sample_log_file), delimiter=\"\\n\", byte_range=(0, size))\n        assert len(s1) &gt;= 1\n</code></pre></li> <li>[ ] Run on WSL2 with GPU to confirm GREEN</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-25-backend-auto-selection-red-green-refactor","title":"Task 2.5 -- Backend Auto-Selection (RED -&gt; GREEN -&gt; REFACTOR)","text":"<ul> <li>[ ] Write <code>tests/unit/test_pipeline.py</code>:   <pre><code>class TestPipeline:\n    def test_should_select_cudf_when_available(self):\n        with patch(\"tensor_grep.core.pipeline.CuDFBackend\") as mock:\n            mock.return_value.is_available.return_value = True\n            pipeline = Pipeline(force_cpu=False)\n            assert pipeline.backend.__class__.__name__ == \"CuDFBackend\"\n\n    def test_should_fallback_to_cpu_when_no_gpu(self):\n        with patch(\"tensor_grep.core.pipeline.CuDFBackend\") as mock:\n            mock.return_value.is_available.return_value = False\n            pipeline = Pipeline(force_cpu=False)\n            assert pipeline.backend.__class__.__name__ == \"CPUBackend\"\n</code></pre></li> <li>[ ] Implement <code>src/tensor_grep/core/pipeline.py</code> (GREEN)</li> <li>[ ] REFACTOR: clean dependency injection</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-26-phase-2-quality-gates","title":"Task 2.6 -- Phase 2 Quality Gates","text":"<ul> <li>[ ] Run full unit + contract + property + acceptance suite</li> <li>[ ] Coverage &gt;= 90%</li> <li>[ ] <code>mutmut run</code> on <code>cudf_backend.py</code> -- &lt; 20% survivors</li> <li>[ ] <code>git commit -m \"Phase 2: cuDF GPU fast path with auto-selection\"</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#phase-3-gpu-memory-management","title":"PHASE 3: GPU Memory Management","text":"<p>Goal: VRAM-safe processing for files larger than GPU memory. Time: ~3 hours</p>"},{"location":"planning/PROJECT_PLAN/#task-31-device-detection-red-green","title":"Task 3.1 -- Device Detection (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/unit/test_device_detect.py</code>:</li> <li>[ ] <code>test_should_detect_no_gpu_when_cuda_unavailable</code> (mocked)</li> <li>[ ] <code>test_should_report_vram_capacity</code> (mocked)</li> <li>[ ] <code>test_should_detect_gds_support</code> (mocked)</li> <li>[ ] <code>test_should_detect_platform</code> (linux/win32/wsl2)</li> <li>[ ] Implement <code>src/tensor_grep/gpu/device_detect.py</code> (GREEN)</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-32-memory-manager-red-green-refactor","title":"Task 3.2 -- Memory Manager (RED -&gt; GREEN -&gt; REFACTOR)","text":"<ul> <li>[ ] Write <code>tests/unit/test_memory_manager.py</code>:</li> <li>[ ] <code>test_should_calculate_chunk_size_from_vram_budget</code></li> <li>[ ] <code>test_should_reserve_20_percent_vram_headroom</code></li> <li>[ ] <code>test_should_recommend_pinned_memory_for_geforce</code></li> <li>[ ] <code>test_should_recommend_gds_for_datacenter_gpu</code></li> <li>[ ] <code>test_should_handle_zero_vram_gracefully</code> (no GPU fallback)</li> <li>[ ] Implement <code>src/tensor_grep/gpu/memory_manager.py</code> (GREEN)</li> <li>[ ] REFACTOR: integrate into CuDFBackend for chunk size auto-tuning</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-33-integration-large-file-processing-red-green","title":"Task 3.3 -- Integration: Large File Processing (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/integration/test_gpu_memory.py</code> (marked <code>@pytest.mark.gpu</code>):</li> <li>[ ] <code>test_should_process_file_larger_than_vram</code> (create 2x VRAM-sized temp file)</li> <li>[ ] <code>test_peak_vram_should_stay_within_budget</code></li> <li>[ ] Run on WSL2 with GPU</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-34-phase-3-quality-gates","title":"Task 3.4 -- Phase 3 Quality Gates","text":"<ul> <li>[ ] Full test suite pass</li> <li>[ ] <code>git commit -m \"Phase 3: VRAM-safe chunked processing with auto-tuning\"</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#phase-4-windows-native-io-path","title":"PHASE 4: Windows Native I/O Path","text":"<p>Goal: dstorage-gpu integration for Windows DirectStorage. Time: ~2 hours</p>"},{"location":"planning/PROJECT_PLAN/#task-41-dstorage-gpu-reader-red-green","title":"Task 4.1 -- dstorage-gpu Reader (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/unit/test_reader_dstorage.py</code> (mocked):</li> <li>[ ] <code>test_should_load_tensor_via_directstorage</code> (mock dstorage_gpu)</li> <li>[ ] <code>test_should_fallback_when_dstorage_unavailable</code></li> <li>[ ] <code>test_should_report_dstorage_available_on_windows</code></li> <li>[ ] Implement <code>src/tensor_grep/io/reader_dstorage.py</code>:   <pre><code>from tensor_grep.io.base import IOBackend\n\nclass DStorageReader(IOBackend):\n    def is_available(self) -&gt; bool:\n        try:\n            import dstorage_gpu\n            import sys\n            return sys.platform == \"win32\"\n        except ImportError:\n            return False\n\n    def read_to_gpu(self, file_path: str):\n        from dstorage_gpu import DirectStorageLoader\n        loader = DirectStorageLoader()\n        return loader.load_tensor(file_path)\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-42-kvikio-reader-red-green","title":"Task 4.2 -- KvikIO Reader (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/unit/test_reader_kvikio.py</code> (mocked):</li> <li>[ ] <code>test_should_read_via_gds_when_available</code></li> <li>[ ] <code>test_should_fallback_to_compat_mode</code></li> <li>[ ] Implement <code>src/tensor_grep/io/reader_kvikio.py</code></li> <li>[ ] Add both to IO contract tests</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-43-phase-4-quality-gates","title":"Task 4.3 -- Phase 4 Quality Gates","text":"<ul> <li>[ ] Full suite pass, coverage &gt;= 90%</li> <li>[ ] <code>git commit -m \"Phase 4: Windows dstorage-gpu + Linux KvikIO I/O paths\"</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#phase-5-nlp-path-cybert-inference","title":"PHASE 5: NLP Path -- cyBERT Inference","text":"<p>Goal: Semantic log classification via cyBERT for \"classify\" / \"detect\" commands. Time: ~5 hours</p>"},{"location":"planning/PROJECT_PLAN/#task-51-acceptance-test-classify-command-red","title":"Task 5.1 -- Acceptance Test: classify command (RED)","text":"<ul> <li>[ ] Write <code>tests/acceptance/test_cli_classify.py</code>:   <pre><code>import subprocess, json, pytest\n\npytestmark = pytest.mark.acceptance\n\nclass TestCLIClassify:\n    def test_should_classify_log_lines(self, sample_log_file):\n        result = subprocess.run(\n            [\"tg\", \"classify\", \"--format\", \"json\", str(sample_log_file)],\n            capture_output=True, text=True,\n        )\n        assert result.returncode == 0\n        data = json.loads(result.stdout)\n        assert \"classifications\" in data\n        assert any(c[\"label\"] in [\"error\", \"info\", \"warning\"] for c in data[\"classifications\"])\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-52-cybert-backend-unit-tests-red","title":"Task 5.2 -- cyBERT Backend Unit Tests (RED)","text":"<ul> <li>[ ] Write <code>tests/unit/test_cybert_backend.py</code> (all mocked):</li> <li>[ ] <code>test_should_tokenize_log_lines</code></li> <li>[ ] <code>test_should_batch_lines_for_inference</code></li> <li>[ ] <code>test_should_classify_with_model_output</code></li> <li>[ ] <code>test_should_extract_entities_from_predictions</code></li> <li>[ ] <code>test_should_handle_model_load_failure_gracefully</code></li> <li>[ ] <code>test_should_report_confidence_scores</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-53-cybert-backend-implementation-green","title":"Task 5.3 -- cyBERT Backend Implementation (GREEN)","text":"<ul> <li>[ ] Implement <code>src/tensor_grep/backends/cybert_backend.py</code>:</li> <li>[ ] Tokenizer wrapper (HuggingFace AutoTokenizer)</li> <li>[ ] Triton client for inference</li> <li>[ ] Postprocessing: labels + entities + confidence</li> <li>[ ] Each sub-component gets its own inner-loop RED-GREEN-REFACTOR cycle</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-54-triton-integration-tests-red-green","title":"Task 5.4 -- Triton Integration Tests (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/integration/test_pipeline_e2e.py</code> (marked <code>@pytest.mark.gpu</code>):</li> <li>[ ] <code>test_full_nlp_pipeline_with_triton</code></li> <li>[ ] <code>test_batch_inference_throughput</code></li> <li>[ ] Requires Triton server running (Docker)</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-55-property-tests-tokenizer-red-green","title":"Task 5.5 -- Property Tests: Tokenizer (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/property/test_tokenizer_props.py</code>:   <pre><code>from hypothesis import given, strategies as st\n\n@given(st.text(min_size=1, max_size=10000, alphabet=st.characters(blacklist_categories=(\"Cs\",))))\ndef test_tokenizer_never_crashes_on_valid_text(text):\n    from tensor_grep.backends.cybert_backend import tokenize\n    tokens = tokenize([text])\n    assert tokens is not None\n    assert len(tokens) &gt; 0\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-56-wire-classify-command-into-cli-green-for-acceptance","title":"Task 5.6 -- Wire classify Command into CLI (GREEN for acceptance)","text":"<ul> <li>[ ] Add <code>classify</code> command to Typer app</li> <li>[ ] Run acceptance test -- confirm GREEN</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-57-phase-5-quality-gates","title":"Task 5.7 -- Phase 5 Quality Gates","text":"<ul> <li>[ ] Full suite pass</li> <li>[ ] <code>mutmut run</code> on <code>cybert_backend.py</code></li> <li>[ ] <code>git commit -m \"Phase 5: cyBERT NLP classification path\"</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#phase-6-performance-benchmarking-optimization","title":"PHASE 6: Performance Benchmarking &amp; Optimization","text":"<p>Goal: Validated performance claims with reproducible benchmarks. Time: ~3 hours</p>"},{"location":"planning/PROJECT_PLAN/#task-61-benchmark-infrastructure-red-green","title":"Task 6.1 -- Benchmark Infrastructure (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/performance/test_throughput.py</code>:   <pre><code>import pytest, time\n\npytestmark = [pytest.mark.slow, pytest.mark.performance]\n\nclass TestThroughput:\n    def test_cpu_backend_throughput(self, tmp_path):\n        \"\"\"Baseline: CPU backend should process &gt;100 MB/s.\"\"\"\n        large = tmp_path / \"large.log\"\n        lines = \"2026-02-24 ERROR test line content here\\n\" * 100_000\n        large.write_text(lines)\n\n        from tensor_grep.backends.cpu_backend import CPUBackend\n        start = time.perf_counter()\n        CPUBackend().search(str(large), \"ERROR\")\n        elapsed = time.perf_counter() - start\n\n        mb = large.stat().st_size / (1024 * 1024)\n        throughput = mb / elapsed\n        assert throughput &gt; 100, f\"CPU throughput {throughput:.1f} MB/s below 100 MB/s\"\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-62-comparative-benchmarks-red-green","title":"Task 6.2 -- Comparative Benchmarks (RED -&gt; GREEN)","text":"<ul> <li>[ ] Write <code>tests/performance/test_vs_ripgrep.py</code>:   <pre><code>import subprocess, time, pytest\n\npytestmark = [pytest.mark.slow, pytest.mark.performance]\n\nclass TestVsRipgrep:\n    def test_semantic_classification_faster_than_multi_rg(self, tmp_path, rg_path):\n        \"\"\"GPU value prop: single classify pass vs N separate rg passes.\"\"\"\n        log = tmp_path / \"mixed.log\"\n        # Generate log with multiple event types\n        lines = []\n        for i in range(10_000):\n            if i % 3 == 0:\n                lines.append(f\"2026-02-24 ERROR Connection timeout from 10.0.0.{i%256}\\n\")\n            elif i % 3 == 1:\n                lines.append(f\"2026-02-24 WARN Disk usage at {60+i%40}%\\n\")\n            else:\n                lines.append(f\"2026-02-24 INFO Request processed in {i%100}ms\\n\")\n        log.write_text(\"\".join(lines))\n\n        patterns = [\"ERROR\", \"WARN\", \"INFO\", r\"\\d+\\.\\d+\\.\\d+\\.\\d+\", \"timeout\", \"Disk usage\"]\n        start = time.perf_counter()\n        for p in patterns:\n            subprocess.run([rg_path, p, str(log)], capture_output=True)\n        rg_total = time.perf_counter() - start\n\n        # Our tool: classify does all at once (when GPU available)\n        start = time.perf_counter()\n        subprocess.run(\n            [\"tg\", \"search\", \"--cpu\", \"ERROR|WARN|INFO\", str(log)],\n            capture_output=True,\n        )\n        our_total = time.perf_counter() - start\n\n        print(f\"ripgrep {len(patterns)} passes: {rg_total:.3f}s\")\n        print(f\"tg single pass: {our_total:.3f}s\")\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-63-ci-benchmark-workflow","title":"Task 6.3 -- CI Benchmark Workflow","text":"<ul> <li>[ ] Create <code>.github/workflows/benchmark.yml</code>:   <pre><code>name: Benchmarks\non:\n  pull_request:\n    paths: ['src/**', 'tests/performance/**']\njobs:\n  benchmark:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-python@v5\n        with: { python-version: '3.11' }\n      - run: pip install -e \".[dev]\"\n      - run: pytest tests/performance/ -v --tb=short\n</code></pre></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-64-phase-6-quality-gates","title":"Task 6.4 -- Phase 6 Quality Gates","text":"<ul> <li>[ ] All benchmarks run without error</li> <li>[ ] Performance numbers documented in test output</li> <li>[ ] <code>git commit -m \"Phase 6: Benchmark infrastructure and baseline measurements\"</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#phase-7-final-integration-polish","title":"PHASE 7: Final Integration &amp; Polish","text":"<p>Goal: All tests green, all quality gates pass, ready for use. Time: ~2 hours</p>"},{"location":"planning/PROJECT_PLAN/#task-71-full-acceptance-suite-green","title":"Task 7.1 -- Full Acceptance Suite (GREEN)","text":"<ul> <li>[ ] Run ALL acceptance tests end-to-end</li> <li>[ ] Fix any remaining failures</li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-72-final-quality-gates","title":"Task 7.2 -- Final Quality Gates","text":"<ul> <li>[ ] <code>pytest --cov=src/tensor_grep -v</code> -- all pass, coverage &gt;= 90%</li> <li>[ ] <code>mutmut run</code> -- &lt; 20% surviving mutants on critical paths</li> <li>[ ] <code>mypy src/</code> -- zero errors</li> <li>[ ] <code>ruff check src/ tests/</code> -- zero errors</li> <li>[ ] <code>pip install -e .</code> and run <code>tg search ERROR tests/test_data/small_sample.log</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#task-73-commit-and-tag","title":"Task 7.3 -- Commit and Tag","text":"<ul> <li>[ ] <code>git add -A &amp;&amp; git commit -m \"v0.1.0: tensor-grep with TDD-driven CPU+GPU dual path\"</code></li> <li>[ ] <code>git tag v0.1.0</code></li> </ul>"},{"location":"planning/PROJECT_PLAN/#tdd-rules-for-this-project-2026-edition","title":"TDD Rules for This Project (2026 Edition)","text":"<ol> <li>No production code without a failing test first. Period.</li> <li>Unit tests run in &lt; 5 seconds total. Mock all GPU/IO. No network. No disk beyond tmp_path.</li> <li>AAA pattern always. Arrange (setup), Act (one call), Assert (one logical assertion).</li> <li>Test names tell a story. <code>test_should_&lt;behavior&gt;_when_&lt;condition&gt;</code>.</li> <li>The test pyramid is law: <pre><code>Acceptance (5-10 tests)    -- minutes to run, outer loop\nIntegration (10-20 tests)  -- require GPU/WSL2, CI nightly\nContract (5-10 tests)      -- verify protocol conformance\nProperty (5-10 tests)      -- Hypothesis, find edge cases\nUnit (50-100 tests)        -- fast, isolated, run on every save\n</code></pre></li> <li>GPU tests are opt-in. Marked <code>@pytest.mark.gpu</code>, auto-skipped when no CUDA.</li> <li>Mutation testing is the real coverage metric. Coverage % is vanity. Mutant kill % is quality.</li> <li>Red-Green-Refactor cycles under 5 minutes. If stuck in red &gt; 5 min, slice smaller.</li> <li>AI assists the inner loop. Use AI to generate edge case tests (Hypothesis strategies), then verify they expose real issues.</li> <li>Characterization tests lock ripgrep parity. Any output format change must update snapshots.</li> </ol>"},{"location":"planning/PROJECT_PLAN/#risk-mitigation","title":"Risk Mitigation","text":"Risk Mitigation Test That Catches It No GPU on dev machine CPU fallback is Phase 1, tested first <code>test_cli_no_gpu.py</code> cuDF not available (Windows) Mocked unit tests + WSL2 integration <code>test_cudf_backend.py</code> (mocked) Morpheus Linux-only cyBERT backend is isolated behind protocol <code>test_backend_contracts.py</code> VRAM exceeded Memory manager auto-chunks <code>test_memory_manager.py</code> PCIe bottleneck Pinned memory + async copy <code>test_gpu_memory.py</code> NVMe SSD is the real bottleneck Throughput claims corrected in matrix <code>test_throughput.py</code> ripgrep faster for simple patterns Fast path uses CPU/cuDF str ops, not NN <code>test_ripgrep_parity.py</code> Test flakiness on GPU Unit tests mock GPU; integration has retry <code>conftest.py</code> skip logic Platform mismatch (win/linux) Platform-aware IO backend selection <code>test_device_detect.py</code> <p>Plan Version: 2.0 Last Updated: 2026-02-24 Method: Outside-In Double-Loop TDD with AI-assisted property testing Phases: 8 (0-7), ~25 hours total estimated effort</p>"},{"location":"planning/RUST_MIGRATION_SUMMARY/","title":"Rust PyO3 Core Backend Migration Summary","text":"<p>We successfully migrated the high-performance Rust core backend (<code>tensor-grep-rs</code>) directly into the main <code>tensor-grep</code> Python package via a <code>PyO3</code> / <code>maturin</code> native extension.</p>"},{"location":"planning/RUST_MIGRATION_SUMMARY/#what-was-achieved","title":"What Was Achieved","text":"<ol> <li>Integrated TDD Tooling &amp; Cargo Configuration:</li> <li>Defined the <code>rust_core</code> folder as a standard Cargo module using <code>pyo3</code> and <code>pyo3-build-config</code>.</li> <li> <p>Adapted <code>pyproject.toml</code> to automatically invoke <code>maturin</code> during <code>uv build</code>, enabling seamless native extension compilation across environments.</p> </li> <li> <p>Rust-to-Python Memory-Safe Bridging:</p> </li> <li>The Rust code was rewritten to stop printing to <code>stdout</code> and instead yield <code>Vec&lt;(usize, String)&gt;</code> representing exact match occurrences (line numbers and exact matched text).</li> <li> <p>This vector safely crosses the PyO3 FFI boundary and maps into Python's native <code>SearchResult</code> representation, preserving full ripgrep parity inside <code>tensor-grep</code>.</p> </li> <li> <p>Pipeline Dynamic Routing:</p> </li> <li>The <code>Pipeline</code> router was updated to dynamically detect the presence of the <code>tensor_grep.rust_core</code> native module.</li> <li> <p>If a GPU is unavailable (or a WSL passthrough failure occurs), <code>tensor-grep</code> immediately skips the slow pure Python regex loop and falls back to the newly integrated <code>RustCoreBackend</code>, maintaining sub-second performance on gigabyte logs.</p> </li> <li> <p>Tested and Verified Parity:</p> </li> <li>The native extension successfully passed all 64 characterization / property-based tests in Pytest.</li> <li>Newline carriage returns (<code>\\r\\n</code>) crossing the boundary were normalized securely without trailing byte-garbage.</li> </ol>"},{"location":"planning/RUST_MIGRATION_SUMMARY/#impact","title":"Impact","text":"<p>Before this integration, CPU fallback (or GPU unavailability) caused a massive penalty via Python's Global Interpreter Lock (GIL) and process spawning overhead (11s+). By mapping Rust memory-mapping (<code>memmap2</code>) and <code>rayon</code> inside the PyO3 wrapper, <code>tensor-grep</code> now guarantees bare-metal regex performance on Windows and Linux equally, without relying on unstable <code>spawn()</code> vs <code>fork()</code> contexts!</p>"},{"location":"planning/RUST_PYO3_PLAN/","title":"Phase 1: Preparation &amp; Rust Refactoring (TDD Baseline)","text":"<ul> <li>[ ] Move <code>tensor-grep-rs</code> contents into <code>tensor-grep/rust_core/</code> (or similar).</li> <li>[ ] Update <code>Cargo.toml</code> to include <code>pyo3</code> and configure <code>crate-type = [\"cdylib\", \"rlib\"]</code> for Python extension building, while maintaining <code>lib.rs</code> architecture for native Rust unit testing.</li> <li>[ ] Ensure existing Rust <code>cargo test</code> suite passes natively before touching Python.</li> </ul>"},{"location":"planning/RUST_PYO3_PLAN/#phase-2-pyo3-bindings-rust-side-tdd","title":"Phase 2: PyO3 Bindings (Rust-Side TDD)","text":"<ul> <li>[ ] Write a Rust unit test that validates the <code>CpuBackend</code> can successfully search a dummy file.</li> <li>[ ] Implement the <code>#[pyclass]</code> and <code>#[pymethods]</code> macros around <code>CpuBackend</code> inside a new <code>src/lib.rs</code> bindings file.</li> <li>[ ] Expose the <code>search</code> function to Python. Ensure it accepts Python strings and paths.</li> </ul>"},{"location":"planning/RUST_PYO3_PLAN/#phase-3-maturin-integration-python-unit-tests-python-side-tdd","title":"Phase 3: Maturin Integration &amp; Python Unit Tests (Python-Side TDD)","text":"<ul> <li>[ ] Update <code>pyproject.toml</code> or create a Maturin build configuration to compile the Rust extension.</li> <li>[ ] Run <code>maturin develop --release</code> to build the Python wheel and install it into the <code>uv</code> virtual environment.</li> <li>[ ] Create a new pytest file: <code>tests/unit/test_rust_core.py</code>.</li> <li>[ ] Write a failing pytest that attempts to <code>import tensor_grep.rust_core</code> and execute <code>CpuBackend.search()</code>.</li> <li>[ ] Validate the test passes using the compiled PyO3 extension.</li> </ul>"},{"location":"planning/RUST_PYO3_PLAN/#phase-4-integration-into-the-router-end-to-end-tdd","title":"Phase 4: Integration into the Router (End-to-End TDD)","text":"<ul> <li>[ ] Update <code>tests/integration/test_pipeline.py</code> to assert that pure string queries are routed to the <code>RustBackend</code>.</li> <li>[ ] Update <code>src/tensor_grep/core/pipeline.py</code> to instantiate the PyO3 <code>RustBackend</code> (replacing the old <code>CPUBackend</code> Python fallback).</li> <li>[ ] Run the full <code>pytest</code> suite (87+ tests) to ensure 100% parity with the previous Python regex behavior.</li> <li>[ ] Run the E2E characterization benchmarks to verify the <code>0.21s</code> speed is maintained through the Python CLI.</li> </ul>"},{"location":"planning/TDD_RUST_PORT_PLAN/","title":"Rust Port Architecture &amp; TDD Plan (v0.4.0)","text":""},{"location":"planning/TDD_RUST_PORT_PLAN/#objective","title":"Objective","text":"<p>Migrate the <code>tensor-grep</code> orchestrator and primary search execution logic from Python to Rust to bypass interpreter startup latency (0ms vs 250ms). This will allow <code>tensor-grep</code> to beat <code>ripgrep</code> globally in all benchmarks, while dynamically calling out to embedded Python for semantic GPU (cuDF/CyBERT) workloads.</p>"},{"location":"planning/TDD_RUST_PORT_PLAN/#the-architecture-shift","title":"The Architecture Shift","text":"<ol> <li>Current: <code>python main.py</code> -&gt; <code>py_pipeline</code> -&gt; (calls rust via PyO3, ripgrep via spawn, or cuDF via Python).</li> <li>Target: <code>rust bin</code> -&gt; <code>rust_pipeline</code> -&gt; (calls raw <code>memchr</code>/<code>regex</code>, or calls Python/cuDF via <code>pyo3::Python::with_gil</code>).</li> </ol>"},{"location":"planning/TDD_RUST_PORT_PLAN/#test-driven-development-tdd-plan","title":"Test-Driven Development (TDD) Plan","text":""},{"location":"planning/TDD_RUST_PORT_PLAN/#phase-1-the-rust-bin-orchestrator","title":"Phase 1: The Rust Bin Orchestrator","text":"<ul> <li>Test: <code>cargo test</code> verifying CLI parsing using <code>clap</code> matches our old Typer definitions exactly.</li> <li>Implement: <code>rust_core/src/main.rs</code> binary entry point, <code>cli.rs</code> config struct.</li> </ul>"},{"location":"planning/TDD_RUST_PORT_PLAN/#phase-2-cpu-core-backends-rust-regex-memchr","title":"Phase 2: CPU Core Backends (Rust <code>regex</code> &amp; <code>memchr</code>)","text":"<ul> <li>Test: Write <code>tests/test_search.rs</code> verifying ripgrep parity exactly for <code>-c</code>, <code>-F</code>, <code>-v</code>, <code>-C</code> switches.</li> <li>Implement: Use the existing <code>rust_core</code> count logic and expand it with full regex lines extraction logic avoiding allocations.</li> </ul>"},{"location":"planning/TDD_RUST_PORT_PLAN/#phase-3-the-python-gpu-bridge-pyo3-embedded","title":"Phase 3: The Python GPU Bridge (<code>pyo3</code> embedded)","text":"<ul> <li>Test: Verify <code>test_gpu_bridge.rs</code> can safely execute our existing <code>cuDFBackend</code> when heavy data is passed.</li> <li>Implement: Use <code>pyo3</code> in reverse. Instead of Python importing Rust as a library, the Rust binary embeds the Python interpreter and dynamically invokes our existing <code>cuDF/CyBERT</code> pipelines only when the GPU heuristic says so.</li> </ul>"},{"location":"planning/TDD_RUST_PORT_PLAN/#phase-4-integration","title":"Phase 4: Integration","text":"<ul> <li>Build a Python wheel via Maturin that includes the standalone Rust binary or exposes it seamlessly.</li> <li>Run <code>benchmarks/run_benchmarks.py</code> to confirm the Rust binary beats ripgrep.</li> </ul>"},{"location":"planning/TDD_v030_PLAN/","title":"Future TDD Implementation Plan (v0.3.0)","text":"<p>Based on 2026 industry standards for AI-assisted Test-Driven Development (TDD) in hybrid Python/Rust ecosystems, we will implement three cutting-edge GPU/SIMD text processing features.</p>"},{"location":"planning/TDD_v030_PLAN/#1-the-2026-tdd-workflow-vibe-coding-with-tests","title":"1. The 2026 TDD Workflow (\"Vibe Coding with Tests\")","text":"<p>Modern TDD emphasizes flow, tight integration with <code>pytest</code> fixtures, and property-based boundary testing.  The cycle we will follow: 1. Write the invariant test first (<code>tests/unit/test_stringzilla.py</code>, <code>test_cudf_jit.py</code>). 2. Implement mocked interfaces to verify routing logic in <code>Pipeline</code>. 3. Build the minimal implementation to pass parity with standard CPU tools. 4. Refactor and optimize via profiling.</p>"},{"location":"planning/TDD_v030_PLAN/#2-feature-1-stringzilla-backend-simd-exact-matching","title":"2. Feature 1: StringZilla Backend (SIMD Exact Matching)","text":"<p>Concept: Research shows <code>StringZilla v4</code> performs exact string matching up to 109x faster than standard libraries on modern hardware via SIMD and CUDA bounds. TDD Steps: - Create <code>tests/unit/test_stringzilla_backend.py</code> asserting it returns identical counts to ripgrep for exact string <code>-F</code> queries. - Implement <code>StringZillaBackend</code> wrapping <code>stringzilla.File</code>. - Update <code>Pipeline</code> to route <code>-F</code> and <code>-c</code> (count) queries to StringZilla when available.</p>"},{"location":"planning/TDD_v030_PLAN/#3-feature-2-cudf-jit-just-in-time-compilation","title":"3. Feature 2: cuDF JIT (Just-in-Time) Compilation","text":"<p>Concept: NVIDIA's latest 2025/2026 whitepapers show that pre-compiled Regex kernels in cuDF are slower than JIT-compiled custom NVRTC text kernels, which achieve 1x-4x speedups. TDD Steps: - Create <code>tests/integration/test_cudf_jit.py</code> validating that string transformations output correctly. - Update <code>CuDFBackend</code> to accept a <code>use_jit=True</code> config flag. - Utilize <code>cudf.core.column.string.compile_regex_jit()</code> (or equivalent NVRTC string kernel builder) for massive data chunks.</p>"},{"location":"planning/TDD_v030_PLAN/#4-feature-3-gpu-ltl-linear-temporal-logic-trace-extraction","title":"4. Feature 3: GPU LTL (Linear Temporal Logic) Trace Extraction","text":"<p>Concept: Recent research (Valizadeh et al., 2024/2026) demonstrates enumerative program synthesis on GPUs evaluating trace logs 2048x faster than CPU. This allows us to search logs not just for text, but for logical sequences (e.g., \"Event A happened before Event B\"). TDD Steps: - Define the CLI interface: <code>tg search --ltl \"A -&gt; eventually B\"</code> - Create <code>tests/unit/test_ltl_backend.py</code>. - Map sequence dependencies to a CUDA/PyTorch trace matrix.</p>"},{"location":"planning/V1_RELEASE_PLAN/","title":"Tensor-Grep v1.0.0 Release Plan (TDD 2026)","text":"<p>This plan outlines the final steps to bring <code>tensor-grep</code> to full 1.0.0 enterprise release parity with ripgrep, utilizing the 2026 standard \"Outside-In Double-Loop TDD\" process. </p>"},{"location":"planning/V1_RELEASE_PLAN/#tdd-core-methodology-2026-best-practices","title":"TDD Core Methodology (2026 Best Practices)","text":"<p>For every phase below, we will follow the rigorous Red-Green-Refactor cycle using the Arrange-Act-Assert (AAA) pattern. </p>"},{"location":"planning/V1_RELEASE_PLAN/#1-the-naming-convention","title":"1. The Naming Convention","text":"<p>Every test must document behavior following the 2026 standard: <code>test_should_[expectedBehavior]_when_[scenario]</code>. Example: <code>test_should_respectIgnoreCase_when_configFlagSet</code></p>"},{"location":"planning/V1_RELEASE_PLAN/#2-the-aaa-pattern","title":"2. The AAA Pattern","text":"<p>Every test will explicitly be broken down into: - Arrange: Set up the exact test data, mock the GPU environments, and configure the <code>SearchConfig</code>. - Act: Execute the backend <code>search()</code> or <code>classify()</code> method. - Assert: Verify the exact <code>SearchResult</code> matches expectations without over-asserting internal implementation details.</p>"},{"location":"planning/V1_RELEASE_PLAN/#3-the-tdd-workflow","title":"3. The TDD Workflow","text":"<ol> <li>Write a failing behavioral unit/integration test (RED).</li> <li>Write the minimum logic in the backend/CLI to satisfy the test (GREEN).</li> <li>Clean up the code, optimize the GPU calls, and ensure types are strictly enforced (REFACTOR).</li> </ol>"},{"location":"planning/V1_RELEASE_PLAN/#phase-1-wire-cli-flags-into-gpu-backends-cudf-cybert","title":"Phase 1: Wire CLI Flags into GPU Backends (cuDF &amp; cyBERT)","text":"<p>While the CPU backend currently respects the <code>SearchConfig</code>, the high-performance GPU backends do not. We need to map ripgrep flags into native RAPIDS operations.</p> <ul> <li>[ ] Step 1.1: cuDF String Operations (Case Insensitivity &amp; Invert)</li> <li>TDD: Write <code>test_should_ignoreCase_when_usingCudfBackend</code> and <code>test_should_invertMatch_when_usingCudfBackend</code>.</li> <li>Implementation: Modify <code>src/tensor_grep/backends/cudf_backend.py</code>. Map <code>config.ignore_case</code> to the <code>flags=re.IGNORECASE</code> equivalent in <code>cudf.Series.str.contains()</code>. Map <code>config.invert_match</code> to bitwise negation <code>~</code> on the resulting boolean mask.</li> <li>[ ] Step 1.2: cyBERT Confidence Filtering</li> <li>TDD: Write <code>test_should_filterConfidence_when_nlpThresholdSet</code>.</li> <li>Implementation: Modify <code>src/tensor_grep/backends/cybert_backend.py</code>. Ensure NLP classification logic drops predictions that don't meet thresholds or respects context line outputs if provided.</li> </ul>"},{"location":"planning/V1_RELEASE_PLAN/#phase-2-implement-advanced-ripgrep-formatting-features","title":"Phase 2: Implement Advanced Ripgrep Formatting Features","text":"<p>Ripgrep is famous for its context awareness and advanced file filtering. We need to implement these inside our core pipelines.</p> <ul> <li>[ ] Step 2.1: Context Lines (-A, -B, -C)</li> <li>TDD: Write <code>test_should_includeAfterContext_when_dashA_isProvided</code>.</li> <li>Implementation: Update the <code>SearchConfig</code> mapping in <code>backends/cpu_backend.py</code> to capture <code>N</code> lines before/after a match, utilizing the <code>context_separator</code> (default <code>--</code>).</li> <li>[ ] Step 2.2: Advanced File Filtering (-g, -t)</li> <li>TDD: Write <code>test_should_filterGlob_when_dashG_provided</code> and <code>test_should_filterType_when_dashT_provided</code>.</li> <li>Implementation: Currently <code>tensor-grep</code> expects a single file path. We need to implement a directory traversal engine in <code>Pipeline</code> or a new <code>io/directory_scanner.py</code> that utilizes <code>config.glob</code> and <code>config.file_type</code> to yield matching file paths before handing them off to the IO readers.</li> </ul>"},{"location":"planning/V1_RELEASE_PLAN/#phase-3-multi-gpu-distribution-and-scaling","title":"Phase 3: Multi-GPU Distribution and Scaling","text":"<p>Currently <code>tensor-grep</code> defaults to <code>cuda:0</code> (a single GPU). To truly leverage enterprise hardware, we need to distribute the parsing workload across multiple available GPUs.</p> <ul> <li>[ ] Step 3.1: Hardware Discovery</li> <li>TDD: Write <code>test_should_detectMultipleGPUs_when_available</code>.</li> <li>Implementation: Update <code>src/tensor_grep/gpu/device_detect.py</code> to return a list of available device IDs (e.g., <code>[0, 1]</code>) instead of just checking device 0.</li> <li>[ ] Step 3.2: Workload Sharding (cuDF &amp; cyBERT)</li> <li>TDD: Write <code>test_should_shardDataAcrossGPUs_when_multiGpuDetected</code>.</li> <li>Implementation: Modify the <code>Pipeline</code> or <code>MemoryManager</code> to chunk incoming files not just by VRAM limits, but to distribute those chunks evenly across a multiprocessing pool or CUDA streams spanning multiple GPUs concurrently.</li> </ul>"},{"location":"planning/V1_RELEASE_PLAN/#phase-4-test-and-trigger-cicd-pipeline","title":"Phase 4: Test and Trigger CI/CD Pipeline","text":"<p>With the software complete, we need to ensure the enterprise GitHub Actions build process works for the standalone Nuitka binaries and Docker containers.</p> <ul> <li>[ ] Step 4.1: Final Integration Test Pass</li> <li>Run the full suite (<code>pytest tests/ -v</code>) to guarantee the advanced features didn't break baseline ripgrep parity. Ensure code coverage remains above 75%.</li> <li>[ ] Step 4.2: Build Distroless GPU Container</li> <li>Ensure <code>Dockerfile.gpu</code> builds cleanly and publishes to GitHub Container Registry (ghcr.io).</li> <li>[ ] Step 4.3: Tag and Trigger</li> <li>Create a git tag <code>v1.0.0-rc1</code> (Release Candidate 1).</li> <li>Push the tag to GitHub (<code>git push origin v1.0.0-rc1</code>) to trigger <code>.github/workflows/release.yml</code>.</li> <li>[ ] Step 4.4: Verify Artifacts</li> <li>Monitor the GitHub Actions run to ensure the Windows, macOS, and Linux standalone binaries compile successfully via Nuitka.</li> </ul>"},{"location":"planning/V1_RELEASE_PLAN/#phase-5-native-package-managers-homebrew-winget","title":"Phase 5: Native Package Managers (Homebrew &amp; Winget)","text":"<p>To act like a true enterprise CLI tool, <code>tg</code> must be installable via native system package managers.</p> <ul> <li>[ ] Step 4.1: Homebrew Formula</li> <li>Create a new repository or directory for the <code>Homebrew Tap</code>.</li> <li>Write <code>tensor-grep.rb</code> outlining the curl download for the macOS binary and installing it to <code>/usr/local/bin/tg</code>.</li> <li>[ ] Step 4.2: Winget Manifest</li> <li>Create the <code>oimiragieo.tensor-grep.yaml</code> manifest file mapping to the Windows <code>.exe</code> artifact generated in Phase 3.</li> <li>Test the manifest locally using <code>winget validate</code>.</li> </ul>"}]}